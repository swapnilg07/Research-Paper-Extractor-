{"Name": "", "mail": "", "Abstract": "", "Introduction": "", "Conclusion": "", "References": ""}{"Name": "", "mail": "", "Abstract": "", "Introduction": "", "Conclusion": "", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}