{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "", "Contents": "\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "", "Contents": "\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "References": ""}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "", "Contents": "\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n\n4 Conclusion\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "Conclusion": "", "References": ""}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n\n4 Conclusion\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "Conclusion": "", "References": ""}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n\n4 Conclusion\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "Conclusion": "", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n\n4 Conclusion\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nReferences\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning\n\nwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-\n\ntions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.\n\nIn Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-\n\nbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan\nRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent\nspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on\n\na short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker\n\nidenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an\nASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,\nJonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.\nInternational Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International\n\nTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng\nYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.\nWeiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive\nspeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.\n\n10\n\n\f", "Conclusion": "", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nRavi Bansal\nTim Bollerslev\nJennifer Francis\nDavid Hsieh\nCharles Lee\nRoni Michaely\nMark Carhart\nStan Levine\nJennifer Lyons\nJim Wicklund\nRalph Goldsticker\nPeter Algert\nCampbell Harvey\nBrett Trueman\nTarget Prices\nPer Firm Number\nPer Firm Panel\nPer Firm\nPercentage Issued\nStrong Buy Buy Hold\nPanel C\nTable II\nTable II Statistics\nTarget Price Target\nMean Max\nStrong Buy Strong Buy Buy Hold\nMarket Reaction", "mail": "", "Abstract": "", "Introduction": "", "Conclusion": "", "References": ""}{"Name": "", "mail": "", "Abstract": "\n\nUsing a large database of analysts\u2019 target prices issued over the period 1997^\n1999, we examine short-term market reactions to target price revisions and\nlong-term comovement of target and stock prices.We \u00a2nd a signi\u00a2cant market\nreaction to the information contained in analysts\u2019 target prices, both uncondi-\ntionally and conditional on contemporaneously issued stock recommendation\nand earnings forecast revisions. Using a cointegration approach, we analyze\nthe long-term behavior of market and target prices. We \u00a2nd that, on average,\nthe one-year-ahead target price is 28 percent higher than the current market\nprice.\n\nACADEMICS, PRACTITIONERS, AND INDIVIDUAL INVESTORS have long been interested in un-\nderstanding the value and usefulness of sell-side analysts\u2019equity reports. In re-\ncent years, security analysts have been increasingly disclosing target prices in\nthese reports, along with their stock recommendations and earnings forecasts.\nThese target prices provide market participants with analysts\u2019 most concise and\nexplicit statement on the magnitude of the \u00a2rm\u2019s expected value. Despite the in-\ncreasing prominence of target prices, their role in conveying information to mar-\nket participants and their contribution to the formation of equity prices have\nremained largely unexplored.1 This paper provides new evidence on these issues.\n\nnBrav is at Duke University and Lehavy is at University of Michigan. We thank Je\u00a1 Abar-\nbanell, Ravi Bansal, Tim Bollerslev, Jennifer Francis, Joel Hasbrouck, David Hsieh, Jack\nHughes, S. P. Kothari, Charles Lee, Roni Michaely, Michael Roberts, seminar participants at\nUniversity of California^Irvine, University of Illinois^Champaign, University of North Caro-\nlina at Chapel Hill, University of Minnesota,Tel-Aviv University, The Interdisciplinary Center\nHerzlyia, Israel, University of Toronto, and Purdue University for their comments, and we\nthank Mark Carhart and Ken French for providing the factor time series. We also thank the\nfollowing individuals for their insights: Stan Levine from First Call, Jennifer Lyons from\nLend Lease Rosen Real Estate Securities, LLC, Jim Wicklund from Dain Rauscher, Inc.,\nRalph Goldsticker from Mellon Capital Management, Len Ya\u00a1e from Bank of America Secu-\nrities, and Peter Algert from Barclays Global Investors. We owe special thanks to John Gra-\nham, Campbell Harvey, Brett Trueman, and Richard Willis for many invaluable insights and\ncomments. All remaining errors are ours.\n\n1 Bradshaw (2002) studies a sample of 103 analysts\u2019 reports and documents the frequency\nwith which analysts employ target prices to justify their choice of recommendations. Using\na sample of 114 Canadian \u00a2rms, Bandyopadhyay, Brown, and Richardson (1995) also \u00a2nd that\nforecasted earnings explain a large proportion of the variation in price forecasts.\n\n1933\n\n\f1934\n\nThe Journal of Finance\n\nUnderstanding the role of target prices in capital markets is important for sev-\neral reasons. First, because target prices are often computed as the product of\nforecasted earnings and a \u00a2nancial ratio such as an earnings yield (Fernandez\n(2001) and Asquith, Mikhail, and Au (2002)), evidence that target prices are infor-\nmative in the presence of earnings forecasts supports the argument that market\nparticipants consider price formation via multiples to be useful. Second, evi-\ndence that market participants react to the information conveyed in analyst tar-\nget prices is relevant for the recent controversy regarding the value of analyst\nresearch reports (see U.S. House of Representatives (2001)). Such evidence should\ntherefore be considered when assessing the implications of potential biases in\nanalysts\u2019opinions on the informativeness of their reports. Third, if target prices\nare incrementally informative, that would suggest that results in prior research\non analysts\u2019 stock recommendations and earnings forecasts might be partially\nattributed to the value that investors assign to price targets. Finally, an investi-\ngation into the role of target prices enables us to evaluate the view that target\nprices provide little or no value to market participants.2 Speci\u00a2cally, it may be\nargued that recommendations and earnings forecasts may completely subsume\nthe information in target prices, since the latter are determined after the stock\nrecommendation and earnings forecast have been set. It may also be argued that\ntarget prices are uninformative and serve as a mere vehicle to enhance an indivi-\ndual analyst\u2019s stature, or that they may not be easily interpreted by investors as\nthey are not necessarily associated with an\u2018\u2018end date.\u2019\u2019 The view that target prices\nprovide little or no value to market participants provides for a natural null hy-\npothesis in this paper.\n\nWe begin our analysis with an examination of stock price reactions both asso-\nciated with and subsequent to target price revisions. If capital market partici-\npants perceive analyst price targets as valuable, we should observe signi\u00a2cant\nprice reactions around their announcements. If larger upward (downward) revi-\nsions in target prices represent more (less) favorable news, we expect market re-\nactions around target price revisions to increase in the favorableness of the\nrevision. Since target prices are generally issued in conjunction with stock re-\ncommendations and earnings forecasts, we also ask whether target prices are\nincrementally informative. Given the discreteness of stock recommendations,\nwe expect target prices to be informative in the presence of stock recommenda-\ntions.\n\nUsing a large database of analyst target prices, we document signi\u00a2cant abnor-\nmal returns around target price revisions and show that the abnormal returns\nare increasing in the favorableness of the target price revision.We also show that\n\n2 O\u2019Brien (2001) re\u00a3ects on the controversy regarding the value of price targets: \u2018\u2018Price tar-\ngets, at their worst, can be used to exploit unsophisticated investorsy. Now that some of the\ndust has settled, market professionals are seeing some marginal value in price targets, if only\nin interpreting the vernacular of Wall Street.\u2019\u2019 Vickers and Weiss (2000) assert that \u2018\u2018yanalysts\nare increasingly lobbing \u2018absurdly extreme\u2019 calls that attract big-media attention and encou-\nrage momentum investing.\u2019\u2019\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1935\n\ntarget prices are incrementally informative, conditional on contemporaneously\nissued stock recommendations and earnings forecast revisions. Motivated by evi-\ndence in prior research of a price\u2018\u2018drift\u2019\u2019subsequent to recommendation and earn-\nings forecast revisions (e.g., Stickel (1995),Womack (1996)), we examine postevent\nabnormal returns.We \u00a2nd that target price revisions contain information regard-\ning future abnormal returns above and beyond that which is conveyed in stock\nrecommendations. This \u00a2nding reinforces the view that target prices do contain\nvaluable information.\n\nFurther evidence on the properties of analyst price targets is provided by an\nanalysis of the long-term comovement of both stock and target prices. Because\ntarget prices are forward looking, we argue that, much like stock prices, they\nought to be linked to the underlying fundamental value of the \u00a2rm. Therefore,\nusing a cointegration framework, we examine the long-term dynamics that link\ntarget and market prices. The ratio of target price to the underlying stock price\nprovides a measure of analysts\u2019 beliefs regarding the \u00a2rm\u2019s expected return. The\ncointegration analysis allows us to estimate the mean of this ratio, which we in-\nterpret as the long-term relation of the two price series.\n\nThe long-term analysis also enables us both to provide evidence on how the\nsystem of target and stock prices reacts to deviations from this long-term relation\nand to quantify the speed and magnitude of adjustment of each price series back\ntoward this long-term relation.We ask whether analysts react to deviations from\nthe long-term relation by adjusting their target prices, or whether stock prices\ncontribute towards most of the long-term adjustments. Given our \u00a2nding of\npostevent excess returns, the long-term analysis is of particular interest because\nit provides evidence as to the relative magnitude by which analysts (investors)\nadjust target (stock) prices toward the long-run target-to-stock price ratio. The\nlong-term analysis, conducted on a subset of 900 \u00a2rms with a continuous target\nprice record, reveals that, on average, target prices are 28 percent higher than\nconcurrent market prices and, moreover, this ratio is inversely related to \u00a2rm\nsize. We also \u00a2nd that once the ratio of target-to-market price is higher (lower)\nthan the estimated long-run ratio, it is primarily analysts who revise their tar-\ngets down (up) such that the ratio reverts back to its long-run value. Market\nprices, in contrast, barely contribute to this correction phase.\n\nIn our \u00a2nal analysis we combine the short- and long-term analyses by examin-\ning whether investors understand the properties of the long-term dynamics that\nwe document. Speci\u00a2cally, for each target price revision, we construct an esti-\nmate of the expected and unexpected component of the revision, and examine\ninvestors\u2019 reactions to each component. We \u00a2nd that average abnormal returns\nare signi\u00a2cantly associated with the proxy for the unexpected revision in the tar-\nget price but not for the expected component.This \u00a2nding supports the view that\ninvestors understand the long-term dynamics that we document.\n\nOur examination of the informativeness of analysts\u2019 target prices contributes\nto extant research on the information content of analysts\u2019 two other signals:\nstock recommendations and earnings forecasts.This research generally \u00a2nds sig-\nni\u00a2cant positive (negative) price reaction to recommendation upgrades (down-\ngrades; e.g., Elton, Gruber, and Grossman (1986), Stickel (1995), Womack (1996)).\n\n\f1936\n\nThe Journal of Finance\n\nRecommendations have also been shown to contain information that is generally\northogonal to the information in other variables known to have predictive power\nfor stock returns (Jegadeesh et al. (2001)). Francis and So\u00a1er (1997) focus on the\nrelative informativeness of analyst earnings forecast revisions and stock recom-\nmendations and \u00a2nd that each signal is informative in the presence of the other,\nwhile Stickel (1999) and Bradshaw (2000) examine the consistency between con-\nsensus recommendations and consensus earnings forecast revisions. We add to\nthis research by examining the value and properties of analysts\u2019 target prices.\nOur combined evidence indicates that target price revisions are informative\nand provide signi\u00a2cant incremental information over and above that contained\nin stock recommendations and earnings forecasts.\n\nThe paper proceeds as follows. Section I describes the data. We examine the\ninformation content of target prices in Section II. Section III describes our coin-\ntegration approach to modeling the long-term comovement of target and stock\nprices.We combine the insights from the short- and long-term analyses in Section\nIV. Conclusions are o\u00a1ered in SectionV.\n\nI. Data and Variable Descriptions\n\nA. Data Description\n\nThe target price, stock recommendation, and earnings forecast databases are\nprovided by First Call.3 We report descriptive statistics in Table I for \u00a2rms with\navailable data on the Center for Research in Security Prices (CRSP) database.\nPanel A of that table provides information on the target price database.The year\n1997 is the \u00a2rst year with complete target price data (coverage begins in Novem-\nber 1996, with 3,862 target price reports for that year). Coverage increases sub-\nstantially over time, from 49,134 target price reports in 1997 to 93,946 reports in\n1999. The average number of price targets per covered \u00a2rm (column 3) also in-\ncreases from 10 in 1997 to 18 in 1999. The target price database is quite compre-\nhensive and includes reports for 6,544 distinct \u00a2rms.The number of participating\nbrokerage houses remains fairly constant over the years, with an increase from\n123 in 1997 to 149 in 1999 (column 5), with 190 distinct brokerage houses issuing\ntarget price reports across all years. Each \u00a2rm in the sample is covered, on aver-\nage, by six brokerage houses. Finally, we \u00a2nd that these \u00a2rms account for approxi-\nmately 93 percent of the total market value of all securities on CRSP.\n\n3 First Call has been a major supplier of analyst data to both practitioners and academics.\nFirst Call maintains that its data collection procedures place great importance on ensuring\naccuracy, especially with respect to the timing of the reports. Consequently, a distinguishing\nfeature of the First Call database is that it codes the source of each analyst\u2019s report as either\n\u2018\u2018real-time\u2019\u2019or \u2018\u2018batch.\u2019\u2019 Real-time refers to reports that are received from live feeds such as the\nbroker notes and that are dated as the date that the report was published. Batch reports are\ngenerated from a weekly batch \u00a2le from the brokerage house, and, hence, their precise pub-\nlication dates are unknown. With technological improvements in First Call\u2019s data collection\nprocedures, by 1999 the overwhelming majority of reports were being coded as real-time. To\nensure accurate dating of analysts\u2019 reports, our empirical analyses include only observations\ncoded as real-time.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1937\n\nPanel B of Table I provides a description of the recommendation database. In\n1997, the database includes 32,295 recommendations for 5,572 distinct \u00a2rms. By\n1999, the number of recommendations reaches 42,014 for 5,929 distinct \u00a2rms. The\n\nTable I\n\nDescriptive Statistics on Analysts\u2019 Target Prices, Stock Recommenda-\n\ntions, and Earnings Forecast Revisions, 1997^1999\n\nThis table reports statistics on the First Call target price (Panel A), stock recommendations\n(Panel B), and earnings forecasts (Panel C) databases, as well as a transition matrix of analyst\nstock recommendations and target prices (Panel D) for \u00a2rms with available data on CRSP. To\nensure accurate dating of analysts\u2019 reports, we include only observations coded as \u2018\u2018real-time\u2019\u2019\n(i.e., reports received from live feeds such as the broker note and that are dated as the date that\nthe report was published). Panels A through C present, by year, the number of observations, the\naverage number of reports per \u00a2rm, the number of \u00a2rms, the number of brokerage houses issuing\nreports, and the average number of brokerage houses per \u00a2rm. The last row in each panel A^C\npresents statistics for the three-year sample period. Panel D presents the number of analyst\nstock recommendations (top number) and the percentage of those recommendations issued\nwith a target price (bottom number), by changes in or reiterations of stock recommendations.\n\nPanel A: Target Prices\n\nPrice Targets\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\nBrokers\n\n(6)\n\nYear\n(1)\n\n1997\n1998\n1999\n\nN\n(2)\n\n49,134\n79,936\n93,946\n\nOverall\n\n223,016\n\nYear\n\n1997\n1998\n1999\n\nOverall\n\nYear\n\n1997\n1998\n1999\n\n32,295\n42,805\n42,014\n\n117,114\n\n39,736\n42,228\n42,322\n\nOverall\n\n124,286\n\n(3)\n\n10\n16\n18\n\n14\n\n6\n7\n7\n\n6\n\n6\n7\n7\n\n7\n\nPanel B: Stock Recommendations\n\nRecommendations\n\nBrokers\n\nN\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\nPanel C: Earnings Forecasts\n\nEarnings Forecasts\n\nBrokers\n\nN\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\n(4)\n\n4,694\n4,997\n5,165\n\n6,544\n\n5,572\n5,871\n5,929\n\n8,673\n\n6,474\n6,203\n6,106\n\n9,167\n\nN\n(5)\n\n123\n136\n149\n\n190\n\nN\n\n211\n222\n210\n\n325\n\nN\n\n204\n233\n246\n\n282\n\n4\n5\n5\n\n6\n\n4\n5\n5\n\n7\n\n5\n5\n5\n\n7\n\n\f1938\n\nThe Journal of Finance\n\nPanel D: Number of Stock Recommendations and Percentage Issued with Target Price\n\nTable I (continued )\n\nFrom Recommendation\n\nStrong Buy\n\nBuy\n\nSell/Strong Sell\n\nStrong Buy\n\nBuy\n\nHold\n\nSell/Strong Sell\n\nNo prior recommendation\n\nOverall\n\nTo Recommendation\n\nHold\n\n3,297\n36%\n5,186\n35%\n12,579\n98%\n424\n41%\n8,622\n49%\n\n30,108\n66%\n\n5,692\n52%\n36,823\n99%\n4,315\n70%\n86\n60%\n15,194\n79%\n\n62,110\n88%\n\n77\n39%\n114\n40%\n427\n42%\n527\n92%\n510\n53%\n\n1,655\n61%\n\n45,671\n99%\n6,108\n60%\n2,485\n71%\n63\n56%\n16,374\n82%\n\n70,701\n91%\n\nnumber of brokerage houses remains fairly constant over the years, with overall\n325 distinct brokerage houses included in the database. Consistent with claims\nmade by several analysts that certain brokerage houses that issue recommenda-\ntions have either a formal or an informal policy barring issuance of price targets,\nthe number of brokerage houses issuing recommendations is higher than those\nissuing price targets.4\n\nPanel C of Table I provides a description of the earnings forecast revision data-\nbase.The database includes 124,286 earnings forecasts for the period from 1997 to\n1999.These forecasts are distributed, on average, as seven forecasts per \u00a2rm and\npertain to 9,167 distinct \u00a2rms.These forecast revisions are issued by 282 distinct\nbrokerage houses, with an average of seven brokers per covered \u00a2rm.\n\nFinally, while analyst reports always include a recommendation, they do not\nnecessarily include a target price. In our sample, 135 of 325 brokerage houses do\nnot issue any target price. Recommendations issued by these 135 brokerage\nhouses, however, account for about \u00a2ve percent of all recommendations.5 Panel\n\n4 In unreported results, we \u00a2nd that the majority of stock recommendations are issued as\neither buy or strong buy (68 percent), while only 29 percent are issued as a hold and three\npercent as a sell or strong sell. The median number of days between revisions is 59 days for\ntarget prices, 141 days for stock recommendations, and 92 days for earnings forecasts.\n\n5 While we do not study the analyst\u2019s decision to include a target price, we conjecture sev-\neral possible reasons. First, according to conversations with analysts, some brokerage houses\nhave an explicit policy prohibiting their analysts from issuing target prices. Second, analysts\nmay choose to withhold the target price in circumstances where their cost of providing an ex\npost incorrect price target exceeds the potential bene\u00a2ts from issuing it. For example, if ana-\nlyst compensation is related to the trading commissions generated in recommending securi-\nties for purchase and if incorrect target prices were ex post costly, then analysts would tend\nto issue target prices mainly with buy rather than with sell recommendations.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1939\n\nD of Table I provides information on the frequency of inclusion of target prices in\nbrokerage houses\u2019 reports. The panel provides a transition matrix of brokerage\nhouse stock recommendations (the number at the top of each cell) and the percen-\ntage of these recommendations issued with price targets (the number at the bot-\ntom of each cell).6 Several interesting regularities are observed in this panel.\nFirst, price targets are overall more likely to be issued along with strong buy or\nbuy recommendations (91percent and 88 percent, respectively) than with hold (66\npercent) or sell/strong sell (61 percent) recommendations.This is consistent with\n\u00a2ndings in Bradshaw (2002). Second, within recommendation categories, recom-\nmendation upgrades (lower-left cells) are more likely to be accompanied by a tar-\nget price than are recommendation downgrades (upper-right cells). For example,\nprice targets are included in 70 percent of the upgrade reports from hold to buy\nrecommendations but only in 35 percent of the downgrade reports from buy to\nhold recommendations. This evidence is consistent with the common claim that\nanalysts are biased toward issuing favorable news and withholding (or minimiz-\ning the amount of) bad news.The statistics on the diagonal indicate that virtually\nall recommendation reiterations include a target price, suggesting that analysts\nconvey new and perhaps more subtle information that does not necessitate a re-\ncommendation revision via target price revisions.\n\nFinally, the statistics in Panel D indicate that analysts are more likely to initi-\nate or resume coverage with a strong buy or a buy recommendation (see McNi-\nchols and O\u2019Brien (1997), Barber et al. (2001)) and are also more likely to include a\ntarget price in these recommendations than with other cases.\n\nB.Variable Descriptions\n\nWe construct two alternative measures for the information content of analysts\u2019\ntarget prices.The \u00a2rst, denotedTP/P, is the ratio of the announced target price to\nthe stock price outstanding two days prior to the announcement (all prices are\nconverted to the same split-adjusted basis). Since more than 90 percent of the\n\n6 In computing these statistics, we employ the following procedures: (1) All recommenda-\ntions outstanding in the database for more than one year are assumed invalid; (2) The most\nrecent brokerage house recommendation is assumed to have been reiterated for target price\nreports that were not accompanied by a corresponding recommendation observation in First\nCall\u2019s recommendation database. The validity of this procedure was con\u00a2rmed with an o\u2044cial\nat First Call who indicated that since target price revisions are issued more frequently than\nrecommendation revisions, many target price revisions are recorded only in the target price\ndatabase and, as long as the corresponding recommendation remains unchanged, First Call\ndoes not reiterate the existing recommendation in the recommendation database (see also\nJegadeesh et al. (2001)); (3) Sell and strong sell recommendations were combined because of\ntheir relative rarity in the data; (4) Since some brokerage houses do not issue target price\nreports, we include only brokerage house/\u00a2rm combinations with at least one target price re-\nport. While results are qualitatively similar, removing the latter restriction reduces the o\u00a1-\ndiagonal percentages. Note also that the transition matrix excludes recommendations marked\nby First Call as revisions from valid to \u2018\u2018dropped.\u2019\u2019 This accounts for the di\u00a1erent number of\nobservations between Panel D and Panel A in Table I.\n\n\f1940\n\nThe Journal of Finance\n\ntarget price reports in the database are coded as one-year-ahead prices, this ratio\nmay be interpreted as the analysts\u2019 stated estimate of the \u00a2rm\u2019s annual expected\nreturn.The second measure attempts to capture whether investors react to infor-\nmation in the announced target price relative to the brokerage house\u2019s prior tar-\nget price.This measure, denoted DTP/P, is the di\u00a1erence between the current and\nprior target price issued by the same brokerage house, de\u00a3ated by stock price out-\nstanding two days prior to the announcement.7\n\nPanel A of Table II presents statistics on the two information measures as well\nas on the target price and earnings forecast revisions. We winsorize these vari-\nables at the 1st and 99th percentiles to mitigate the possible e\u00a1ect of extreme ob-\nservations. The statistics indicate that the distributions of both measures are\nright skewed. The average (median) target price is higher by 32.9 (25.5) percent\nrelative to the preannouncement stock price. As a percentage of stock price, in-\ndividual brokerage houses\u2019 target prices are 0.8 percent higher than the previous\ntarget price.8 The third column presents additional information on the change in\nthe brokerage house target price, scaling it in this case by the brokerage house\nprevious target price, DTP/TP (cid:2) 1.The average (median) percentage change in tar-\nget price is 5.3 (0) percent. Finally, in the fourth column we report summary sta-\ntistics for the earnings forecast revision measured as the change in the analyst\nforecast of earnings for the current \u00a2scal year de\u00a3ated by the stock price two days\nprior to the announcement.The mean (median) forecast revision is (cid:2) 0.41 ((cid:2) 0.03)\npercent.\n\nPanels B and C of Table II present additional information both for the level and\nchange in target prices conditioned on the associated recommendation revision.\nIn Panel B we report average target prices scaled by preannouncement stock\nprice, TP/P. In general, the magnitude of the scaled target prices is consistent\nwith the direction of the recommendation changes. For example, upgrades are\ngenerally associated with higher TP/P ratios than downgrades. Next, in Panel C\nwe report for each recommendation revision averages of DTP/TP (cid:2) 1 as well as\naverage price appreciation over the same period (since the issuance of the preced-\ning target price). It can be seen that the average DTP/TP (cid:2) 1 and the stock price\nappreciation are consistently positive for upgrades and nearly always negative\nfor downgrades. For example, an upgrade from a buy to a strong buy recommen-\ndation is associated with an average upward revision in DTP/TP (cid:2) 1 of 12.7 per-\ncent, whereas a downgrade from a buy to a hold recommendation is associated\n\n7 We have also considered additional measures. The \u00a2rst is the di\u00a1erence between a broker-\nage house\u2019s target price and the outstanding consensus target price immediately prior to the\nannouncement. Consensus target price was calculated as the average target price outstand-\ning over the previous 90 days across all brokerage houses. Other information measures are\nconstructed by scaling each of the previous target price revisions by the prior-price standard\ndeviation, measured over the 90 days preceding the event. We \u00a2nd qualitatively similar results\nin Section II with all of these information measures.\n\n8 In unreported results, we \u00a2nd that only about \u00a2ve percent of target price reports are is-\nsued below the concurrent stock price, approximately 25 percent of target price reports re-\n\u00a3ect a downward revision from brokerage houses\u2019 prior reports, and nearly 43 percent of\ntarget price reports re\u00a3ect a downward revision from the outstanding consensus target price.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1941\nwith a downward revision of (cid:2) 4.5 percent on average. Similarly, the average\nprice appreciation over the period preceding the announcement is also consis-\ntent with the direction of the recommendation and target price revisions. For\nthe upgrade from a buy to a strong buy recommendation, the associated stock\nprice appreciation is 5.1 percent, whereas for the downgrade from a buy to a hold\n\nTable II\n\nStatistics on Target Prices byAnalyst Stock Recommendations\n\nThis table provides descriptive statistics on the target price information measures. Panel A pro-\nvides general distributional statistics on (a) the ratio of target price to preannouncement stock\nprice (stock price outstanding two days prior to the announcement of the target price), denoted\n(TP/P), (b) the change in the individual brokerage house target price scaled by preannounce-\nment stock price, denoted (DTP/P), (c) the percentage change in the brokerage house target\nprice, denoted (DTP/TP(cid:2) 1), and (d) earnings forecast revision, computed as the di\u00a1erence in\nthe brokerage house current and prior annual earnings forecast scaled by preannouncement\nstock price. Panel B provides information on the average TP/P conditional on stock recommen-\ndation revisions. Panel C reports, for each recommendation revision, averages of DTP/TP(cid:2) 1 as\nwell as average price appreciation measured over the same period (since the issuance of the\npreceding target price). All prices and earnings are converted to the same split-adjusted basis.\n\nPanel A: Descriptive Statistics on Measures of the Information Content of Target Price\n\nTarget Price to\n\nStock Price\n\nRatio\n(TP/P)\n\nMean\nMax\n\n75th percentile\n\nMedian\n\n25th percentile\n\nMin\n\nStd. Dev.\n\nN\n\n1.329\n3.004\n1.433\n1.255\n1.146\n0.584\n0.304\n204,031\n\nChange in\nBrokerage\n\nHouse\nTarget\nPrice\n\n(DTP/P)\n\nChange in\nBrokerage\n\nHouse\nTarget\nPrice\n\n(DTP/IP(cid:2) 1)\n\n0.8%\n143.3%\n9.7%\n0.0%\n0.0%\n(cid:2) 136.0%\n78.3%\n115,720\n\n5.3%\n183.3%\n8.3%\n0.0%\n(cid:2) 0.8%\n(cid:2) 89.0%\n95.9%\n115,720\n\nPanel B: Average Target Price to Price Ratio (TP/P)\n\nTo Recommendation\n\nFrom\n\nRecommendation\n\nStrong\nBuy\n\nStrong Buy\n\nBuy\nHold\n\nSell/Strong Sell\n\nInitiated/Resumed as\n\nOverall\n\n1.40\n1.41\n1.37\n1.42\n1.43\n\n1.41\n\nBuy\n\n1.30\n1.31\n1.31\n1.36\n1.31\n\n1.31\n\nHold\n\n1.18\n1.12\n1.16\n1.11\n1.15\n\n1.16\n\nForecast\nRevision\n(cid:2) 0.41%\n35.2%\n0.16%\n(cid:2) 0.03%\n(cid:2) 0.43%\n(cid:2) 422.5%\n2.5%\n82,052\n\nSell/Strong\n\nSell\n\n1.04\n1.21\n1.01\n1.03\n1.07\n\n1.04\n\n\f1942\n\nThe Journal of Finance\n\nPanel C: Average Change in Target Price (DTP/TP (cid:2) 1) and Corresponding Price Appreciation\n\nTable II (continued )\n\nFrom\n\nRecommendation\n\nStrong\nBuy\n\nStrong Buy\n\nBuy\nHold\n\nSell/Strong Sell\nInitiated/Resumed as\n\n6.0%,\n12.7%,\n22.8%,\n20.6%,\n\n5.7%\n5.1%\n4.6%\n1.6%\n\nNA\n\nOverall\n\n6.6%,\n\n5.6%\n\n5.7%,\n\nTo Recommendation\n\nBuy\n\nHold\n\nSell/Strong\n\nSell\n\n6.4%,\n5.4%,\n16.4%,\n15.6%,\n\n2.5% (cid:2) 9.9%, (cid:2)0.2% (cid:2) 14.5%, (cid:2) 3.5%\n4.3% (cid:2) 4.5%,\n2.4% (cid:2) 6.1%,\n5.9%\n0.01% (cid:2) 7.2%, (cid:2) 2.9%\n4.4% 0.6%,\n0.5%, (cid:2) 3.5%\n2.1% 11.0%,\n0.7%\n\nNA\n\nNA\n4.2% (cid:2) 0.4%,\n\nNA\n\n0.3% (cid:2) 1.7%, (cid:2) 2.9\n\nrecommendation, stock prices appreciated on average by 2.4 percent.9 Finally, we\nnote that in the case of recommendation reiterations, the magnitude of target\nprice revisions is lower than in recommendation upgrades or downgrades.\n\nWe have also calculated statistics, as in Panels B and C, for the variation in\nearnings revisions by stock recommendation revisions (unreported). We \u00a2nd\nthat, similar to the results in these panels, earnings revisions are monotonically\nrelated to the favorableness of the recommendation change. The fact that revi-\nsions in target prices, recommendations, and earnings forecasts occur generally\nin the same direction suggests that, to some extent, these signals share much of\nthe same information content. In Section II we explore whether the information\nin each of these signals subsumes the information in any other.\n\nII. Market Reaction to Target Price Announcements\n\nA. Unconditional Informativeness of Target Prices\n\nIn this section, we examine whether the information content of target price\nannouncements is associated with abnormal returns around those announce-\nments. Speci\u00a2cally, we compute the abnormal return around each announcement\nand present average abnormal returns for portfolios ranked on the basis of the\nmagnitude of the relevant information content measure. Abnormal return is com-\nputed as the di\u00a1erence between a \u00a2rm\u2019s buy-and-hold return and the buy-and-hold\nreturn on the NYSE/AMEX/Nasdaq value-weighted market index over the period\nbeginning two days prior and ending two days subsequent to the \u00a2rm\u2019s target\nprice announcement.10 These results are reported in Figure 1.\n\nproximately three percent.\n\n9 The average contemporaneous market return for all recommendation categories is ap-\n10 Results for the period of (cid:2) 1 to \u00fe 1 days around the announcement are qualitatively simi-\nlar. Also, to avoid possible cross-correlation problems caused by identical return observations,\nwe delete all but one of identical return observations within each portfolio.\n\n\f", "Introduction": "", "Conclusion": "", "References": ""}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nZhejiang University\nSheng Zhao Microsoft", "mail": "rayeren@zju.edu.cn,ruanyj3107@zju.edu.cn,xuta@microsoft.com,taoqin@microsoft.com,Sheng.Zhao@microsoft.com,zhaozhou@zju.edu.cn,tyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "", "mail": "", "Abstract": "\n\nUsing a large database of analysts\u2019 target prices issued over the period 1997^\n1999, we examine short-term market reactions to target price revisions and\nlong-term comovement of target and stock prices.We \u00a2nd a signi\u00a2cant market\nreaction to the information contained in analysts\u2019 target prices, both uncondi-\ntionally and conditional on contemporaneously issued stock recommendation\nand earnings forecast revisions. Using a cointegration approach, we analyze\nthe long-term behavior of market and target prices. We \u00a2nd that, on average,\nthe one-year-ahead target price is 28 percent higher than the current market\nprice.\n\nACADEMICS, PRACTITIONERS, AND INDIVIDUAL INVESTORS have long been interested in un-\nderstanding the value and usefulness of sell-side analysts\u2019equity reports. In re-\ncent years, security analysts have been increasingly disclosing target prices in\nthese reports, along with their stock recommendations and earnings forecasts.\nThese target prices provide market participants with analysts\u2019 most concise and\nexplicit statement on the magnitude of the \u00a2rm\u2019s expected value. Despite the in-\ncreasing prominence of target prices, their role in conveying information to mar-\nket participants and their contribution to the formation of equity prices have\nremained largely unexplored.1 This paper provides new evidence on these issues.\n\nnBrav is at Duke University and Lehavy is at University of Michigan. We thank Je\u00a1 Abar-\nbanell, Ravi Bansal, Tim Bollerslev, Jennifer Francis, Joel Hasbrouck, David Hsieh, Jack\nHughes, S. P. Kothari, Charles Lee, Roni Michaely, Michael Roberts, seminar participants at\nUniversity of California^Irvine, University of Illinois^Champaign, University of North Caro-\nlina at Chapel Hill, University of Minnesota,Tel-Aviv University, The Interdisciplinary Center\nHerzlyia, Israel, University of Toronto, and Purdue University for their comments, and we\nthank Mark Carhart and Ken French for providing the factor time series. We also thank the\nfollowing individuals for their insights: Stan Levine from First Call, Jennifer Lyons from\nLend Lease Rosen Real Estate Securities, LLC, Jim Wicklund from Dain Rauscher, Inc.,\nRalph Goldsticker from Mellon Capital Management, Len Ya\u00a1e from Bank of America Secu-\nrities, and Peter Algert from Barclays Global Investors. We owe special thanks to John Gra-\nham, Campbell Harvey, Brett Trueman, and Richard Willis for many invaluable insights and\ncomments. All remaining errors are ours.\n\n1 Bradshaw (2002) studies a sample of 103 analysts\u2019 reports and documents the frequency\nwith which analysts employ target prices to justify their choice of recommendations. Using\na sample of 114 Canadian \u00a2rms, Bandyopadhyay, Brown, and Richardson (1995) also \u00a2nd that\nforecasted earnings explain a large proportion of the variation in price forecasts.\n\n1933\n\n\f1934\n\nThe Journal of Finance\n\nUnderstanding the role of target prices in capital markets is important for sev-\neral reasons. First, because target prices are often computed as the product of\nforecasted earnings and a \u00a2nancial ratio such as an earnings yield (Fernandez\n(2001) and Asquith, Mikhail, and Au (2002)), evidence that target prices are infor-\nmative in the presence of earnings forecasts supports the argument that market\nparticipants consider price formation via multiples to be useful. Second, evi-\ndence that market participants react to the information conveyed in analyst tar-\nget prices is relevant for the recent controversy regarding the value of analyst\nresearch reports (see U.S. House of Representatives (2001)). Such evidence should\ntherefore be considered when assessing the implications of potential biases in\nanalysts\u2019opinions on the informativeness of their reports. Third, if target prices\nare incrementally informative, that would suggest that results in prior research\non analysts\u2019 stock recommendations and earnings forecasts might be partially\nattributed to the value that investors assign to price targets. Finally, an investi-\ngation into the role of target prices enables us to evaluate the view that target\nprices provide little or no value to market participants.2 Speci\u00a2cally, it may be\nargued that recommendations and earnings forecasts may completely subsume\nthe information in target prices, since the latter are determined after the stock\nrecommendation and earnings forecast have been set. It may also be argued that\ntarget prices are uninformative and serve as a mere vehicle to enhance an indivi-\ndual analyst\u2019s stature, or that they may not be easily interpreted by investors as\nthey are not necessarily associated with an\u2018\u2018end date.\u2019\u2019 The view that target prices\nprovide little or no value to market participants provides for a natural null hy-\npothesis in this paper.\n\nWe begin our analysis with an examination of stock price reactions both asso-\nciated with and subsequent to target price revisions. If capital market partici-\npants perceive analyst price targets as valuable, we should observe signi\u00a2cant\nprice reactions around their announcements. If larger upward (downward) revi-\nsions in target prices represent more (less) favorable news, we expect market re-\nactions around target price revisions to increase in the favorableness of the\nrevision. Since target prices are generally issued in conjunction with stock re-\ncommendations and earnings forecasts, we also ask whether target prices are\nincrementally informative. Given the discreteness of stock recommendations,\nwe expect target prices to be informative in the presence of stock recommenda-\ntions.\n\nUsing a large database of analyst target prices, we document signi\u00a2cant abnor-\nmal returns around target price revisions and show that the abnormal returns\nare increasing in the favorableness of the target price revision.We also show that\n\n2 O\u2019Brien (2001) re\u00a3ects on the controversy regarding the value of price targets: \u2018\u2018Price tar-\ngets, at their worst, can be used to exploit unsophisticated investorsy. Now that some of the\ndust has settled, market professionals are seeing some marginal value in price targets, if only\nin interpreting the vernacular of Wall Street.\u2019\u2019 Vickers and Weiss (2000) assert that \u2018\u2018yanalysts\nare increasingly lobbing \u2018absurdly extreme\u2019 calls that attract big-media attention and encou-\nrage momentum investing.\u2019\u2019\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1935\n\ntarget prices are incrementally informative, conditional on contemporaneously\nissued stock recommendations and earnings forecast revisions. Motivated by evi-\ndence in prior research of a price\u2018\u2018drift\u2019\u2019subsequent to recommendation and earn-\nings forecast revisions (e.g., Stickel (1995),Womack (1996)), we examine postevent\nabnormal returns.We \u00a2nd that target price revisions contain information regard-\ning future abnormal returns above and beyond that which is conveyed in stock\nrecommendations. This \u00a2nding reinforces the view that target prices do contain\nvaluable information.\n\nFurther evidence on the properties of analyst price targets is provided by an\nanalysis of the long-term comovement of both stock and target prices. Because\ntarget prices are forward looking, we argue that, much like stock prices, they\nought to be linked to the underlying fundamental value of the \u00a2rm. Therefore,\nusing a cointegration framework, we examine the long-term dynamics that link\ntarget and market prices. The ratio of target price to the underlying stock price\nprovides a measure of analysts\u2019 beliefs regarding the \u00a2rm\u2019s expected return. The\ncointegration analysis allows us to estimate the mean of this ratio, which we in-\nterpret as the long-term relation of the two price series.\n\nThe long-term analysis also enables us both to provide evidence on how the\nsystem of target and stock prices reacts to deviations from this long-term relation\nand to quantify the speed and magnitude of adjustment of each price series back\ntoward this long-term relation.We ask whether analysts react to deviations from\nthe long-term relation by adjusting their target prices, or whether stock prices\ncontribute towards most of the long-term adjustments. Given our \u00a2nding of\npostevent excess returns, the long-term analysis is of particular interest because\nit provides evidence as to the relative magnitude by which analysts (investors)\nadjust target (stock) prices toward the long-run target-to-stock price ratio. The\nlong-term analysis, conducted on a subset of 900 \u00a2rms with a continuous target\nprice record, reveals that, on average, target prices are 28 percent higher than\nconcurrent market prices and, moreover, this ratio is inversely related to \u00a2rm\nsize. We also \u00a2nd that once the ratio of target-to-market price is higher (lower)\nthan the estimated long-run ratio, it is primarily analysts who revise their tar-\ngets down (up) such that the ratio reverts back to its long-run value. Market\nprices, in contrast, barely contribute to this correction phase.\n\nIn our \u00a2nal analysis we combine the short- and long-term analyses by examin-\ning whether investors understand the properties of the long-term dynamics that\nwe document. Speci\u00a2cally, for each target price revision, we construct an esti-\nmate of the expected and unexpected component of the revision, and examine\ninvestors\u2019 reactions to each component. We \u00a2nd that average abnormal returns\nare signi\u00a2cantly associated with the proxy for the unexpected revision in the tar-\nget price but not for the expected component.This \u00a2nding supports the view that\ninvestors understand the long-term dynamics that we document.\n\nOur examination of the informativeness of analysts\u2019 target prices contributes\nto extant research on the information content of analysts\u2019 two other signals:\nstock recommendations and earnings forecasts.This research generally \u00a2nds sig-\nni\u00a2cant positive (negative) price reaction to recommendation upgrades (down-\ngrades; e.g., Elton, Gruber, and Grossman (1986), Stickel (1995), Womack (1996)).\n\n\f1936\n\nThe Journal of Finance\n\nRecommendations have also been shown to contain information that is generally\northogonal to the information in other variables known to have predictive power\nfor stock returns (Jegadeesh et al. (2001)). Francis and So\u00a1er (1997) focus on the\nrelative informativeness of analyst earnings forecast revisions and stock recom-\nmendations and \u00a2nd that each signal is informative in the presence of the other,\nwhile Stickel (1999) and Bradshaw (2000) examine the consistency between con-\nsensus recommendations and consensus earnings forecast revisions. We add to\nthis research by examining the value and properties of analysts\u2019 target prices.\nOur combined evidence indicates that target price revisions are informative\nand provide signi\u00a2cant incremental information over and above that contained\nin stock recommendations and earnings forecasts.\n\nThe paper proceeds as follows. Section I describes the data. We examine the\ninformation content of target prices in Section II. Section III describes our coin-\ntegration approach to modeling the long-term comovement of target and stock\nprices.We combine the insights from the short- and long-term analyses in Section\nIV. Conclusions are o\u00a1ered in SectionV.\n\nI. Data and Variable Descriptions\n\nA. Data Description\n\nThe target price, stock recommendation, and earnings forecast databases are\nprovided by First Call.3 We report descriptive statistics in Table I for \u00a2rms with\navailable data on the Center for Research in Security Prices (CRSP) database.\nPanel A of that table provides information on the target price database.The year\n1997 is the \u00a2rst year with complete target price data (coverage begins in Novem-\nber 1996, with 3,862 target price reports for that year). Coverage increases sub-\nstantially over time, from 49,134 target price reports in 1997 to 93,946 reports in\n1999. The average number of price targets per covered \u00a2rm (column 3) also in-\ncreases from 10 in 1997 to 18 in 1999. The target price database is quite compre-\nhensive and includes reports for 6,544 distinct \u00a2rms.The number of participating\nbrokerage houses remains fairly constant over the years, with an increase from\n123 in 1997 to 149 in 1999 (column 5), with 190 distinct brokerage houses issuing\ntarget price reports across all years. Each \u00a2rm in the sample is covered, on aver-\nage, by six brokerage houses. Finally, we \u00a2nd that these \u00a2rms account for approxi-\nmately 93 percent of the total market value of all securities on CRSP.\n\n3 First Call has been a major supplier of analyst data to both practitioners and academics.\nFirst Call maintains that its data collection procedures place great importance on ensuring\naccuracy, especially with respect to the timing of the reports. Consequently, a distinguishing\nfeature of the First Call database is that it codes the source of each analyst\u2019s report as either\n\u2018\u2018real-time\u2019\u2019or \u2018\u2018batch.\u2019\u2019 Real-time refers to reports that are received from live feeds such as the\nbroker notes and that are dated as the date that the report was published. Batch reports are\ngenerated from a weekly batch \u00a2le from the brokerage house, and, hence, their precise pub-\nlication dates are unknown. With technological improvements in First Call\u2019s data collection\nprocedures, by 1999 the overwhelming majority of reports were being coded as real-time. To\nensure accurate dating of analysts\u2019 reports, our empirical analyses include only observations\ncoded as real-time.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1937\n\nPanel B of Table I provides a description of the recommendation database. In\n1997, the database includes 32,295 recommendations for 5,572 distinct \u00a2rms. By\n1999, the number of recommendations reaches 42,014 for 5,929 distinct \u00a2rms. The\n\nTable I\n\nDescriptive Statistics on Analysts\u2019 Target Prices, Stock Recommenda-\n\ntions, and Earnings Forecast Revisions, 1997^1999\n\nThis table reports statistics on the First Call target price (Panel A), stock recommendations\n(Panel B), and earnings forecasts (Panel C) databases, as well as a transition matrix of analyst\nstock recommendations and target prices (Panel D) for \u00a2rms with available data on CRSP. To\nensure accurate dating of analysts\u2019 reports, we include only observations coded as \u2018\u2018real-time\u2019\u2019\n(i.e., reports received from live feeds such as the broker note and that are dated as the date that\nthe report was published). Panels A through C present, by year, the number of observations, the\naverage number of reports per \u00a2rm, the number of \u00a2rms, the number of brokerage houses issuing\nreports, and the average number of brokerage houses per \u00a2rm. The last row in each panel A^C\npresents statistics for the three-year sample period. Panel D presents the number of analyst\nstock recommendations (top number) and the percentage of those recommendations issued\nwith a target price (bottom number), by changes in or reiterations of stock recommendations.\n\nPanel A: Target Prices\n\nPrice Targets\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\nBrokers\n\n(6)\n\nYear\n(1)\n\n1997\n1998\n1999\n\nN\n(2)\n\n49,134\n79,936\n93,946\n\nOverall\n\n223,016\n\nYear\n\n1997\n1998\n1999\n\nOverall\n\nYear\n\n1997\n1998\n1999\n\n32,295\n42,805\n42,014\n\n117,114\n\n39,736\n42,228\n42,322\n\nOverall\n\n124,286\n\n(3)\n\n10\n16\n18\n\n14\n\n6\n7\n7\n\n6\n\n6\n7\n7\n\n7\n\nPanel B: Stock Recommendations\n\nRecommendations\n\nBrokers\n\nN\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\nPanel C: Earnings Forecasts\n\nEarnings Forecasts\n\nBrokers\n\nN\n\nAvg. No. Per Firm\n\nNumber of Firms\n\nAvg. No. Per Firm\n\n(4)\n\n4,694\n4,997\n5,165\n\n6,544\n\n5,572\n5,871\n5,929\n\n8,673\n\n6,474\n6,203\n6,106\n\n9,167\n\nN\n(5)\n\n123\n136\n149\n\n190\n\nN\n\n211\n222\n210\n\n325\n\nN\n\n204\n233\n246\n\n282\n\n4\n5\n5\n\n6\n\n4\n5\n5\n\n7\n\n5\n5\n5\n\n7\n\n\f1938\n\nThe Journal of Finance\n\nPanel D: Number of Stock Recommendations and Percentage Issued with Target Price\n\nTable I (continued )\n\nFrom Recommendation\n\nStrong Buy\n\nBuy\n\nSell/Strong Sell\n\nStrong Buy\n\nBuy\n\nHold\n\nSell/Strong Sell\n\nNo prior recommendation\n\nOverall\n\nTo Recommendation\n\nHold\n\n3,297\n36%\n5,186\n35%\n12,579\n98%\n424\n41%\n8,622\n49%\n\n30,108\n66%\n\n5,692\n52%\n36,823\n99%\n4,315\n70%\n86\n60%\n15,194\n79%\n\n62,110\n88%\n\n77\n39%\n114\n40%\n427\n42%\n527\n92%\n510\n53%\n\n1,655\n61%\n\n45,671\n99%\n6,108\n60%\n2,485\n71%\n63\n56%\n16,374\n82%\n\n70,701\n91%\n\nnumber of brokerage houses remains fairly constant over the years, with overall\n325 distinct brokerage houses included in the database. Consistent with claims\nmade by several analysts that certain brokerage houses that issue recommenda-\ntions have either a formal or an informal policy barring issuance of price targets,\nthe number of brokerage houses issuing recommendations is higher than those\nissuing price targets.4\n\nPanel C of Table I provides a description of the earnings forecast revision data-\nbase.The database includes 124,286 earnings forecasts for the period from 1997 to\n1999.These forecasts are distributed, on average, as seven forecasts per \u00a2rm and\npertain to 9,167 distinct \u00a2rms.These forecast revisions are issued by 282 distinct\nbrokerage houses, with an average of seven brokers per covered \u00a2rm.\n\nFinally, while analyst reports always include a recommendation, they do not\nnecessarily include a target price. In our sample, 135 of 325 brokerage houses do\nnot issue any target price. Recommendations issued by these 135 brokerage\nhouses, however, account for about \u00a2ve percent of all recommendations.5 Panel\n\n4 In unreported results, we \u00a2nd that the majority of stock recommendations are issued as\neither buy or strong buy (68 percent), while only 29 percent are issued as a hold and three\npercent as a sell or strong sell. The median number of days between revisions is 59 days for\ntarget prices, 141 days for stock recommendations, and 92 days for earnings forecasts.\n\n5 While we do not study the analyst\u2019s decision to include a target price, we conjecture sev-\neral possible reasons. First, according to conversations with analysts, some brokerage houses\nhave an explicit policy prohibiting their analysts from issuing target prices. Second, analysts\nmay choose to withhold the target price in circumstances where their cost of providing an ex\npost incorrect price target exceeds the potential bene\u00a2ts from issuing it. For example, if ana-\nlyst compensation is related to the trading commissions generated in recommending securi-\nties for purchase and if incorrect target prices were ex post costly, then analysts would tend\nto issue target prices mainly with buy rather than with sell recommendations.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1939\n\nD of Table I provides information on the frequency of inclusion of target prices in\nbrokerage houses\u2019 reports. The panel provides a transition matrix of brokerage\nhouse stock recommendations (the number at the top of each cell) and the percen-\ntage of these recommendations issued with price targets (the number at the bot-\ntom of each cell).6 Several interesting regularities are observed in this panel.\nFirst, price targets are overall more likely to be issued along with strong buy or\nbuy recommendations (91percent and 88 percent, respectively) than with hold (66\npercent) or sell/strong sell (61 percent) recommendations.This is consistent with\n\u00a2ndings in Bradshaw (2002). Second, within recommendation categories, recom-\nmendation upgrades (lower-left cells) are more likely to be accompanied by a tar-\nget price than are recommendation downgrades (upper-right cells). For example,\nprice targets are included in 70 percent of the upgrade reports from hold to buy\nrecommendations but only in 35 percent of the downgrade reports from buy to\nhold recommendations. This evidence is consistent with the common claim that\nanalysts are biased toward issuing favorable news and withholding (or minimiz-\ning the amount of) bad news.The statistics on the diagonal indicate that virtually\nall recommendation reiterations include a target price, suggesting that analysts\nconvey new and perhaps more subtle information that does not necessitate a re-\ncommendation revision via target price revisions.\n\nFinally, the statistics in Panel D indicate that analysts are more likely to initi-\nate or resume coverage with a strong buy or a buy recommendation (see McNi-\nchols and O\u2019Brien (1997), Barber et al. (2001)) and are also more likely to include a\ntarget price in these recommendations than with other cases.\n\nB.Variable Descriptions\n\nWe construct two alternative measures for the information content of analysts\u2019\ntarget prices.The \u00a2rst, denotedTP/P, is the ratio of the announced target price to\nthe stock price outstanding two days prior to the announcement (all prices are\nconverted to the same split-adjusted basis). Since more than 90 percent of the\n\n6 In computing these statistics, we employ the following procedures: (1) All recommenda-\ntions outstanding in the database for more than one year are assumed invalid; (2) The most\nrecent brokerage house recommendation is assumed to have been reiterated for target price\nreports that were not accompanied by a corresponding recommendation observation in First\nCall\u2019s recommendation database. The validity of this procedure was con\u00a2rmed with an o\u2044cial\nat First Call who indicated that since target price revisions are issued more frequently than\nrecommendation revisions, many target price revisions are recorded only in the target price\ndatabase and, as long as the corresponding recommendation remains unchanged, First Call\ndoes not reiterate the existing recommendation in the recommendation database (see also\nJegadeesh et al. (2001)); (3) Sell and strong sell recommendations were combined because of\ntheir relative rarity in the data; (4) Since some brokerage houses do not issue target price\nreports, we include only brokerage house/\u00a2rm combinations with at least one target price re-\nport. While results are qualitatively similar, removing the latter restriction reduces the o\u00a1-\ndiagonal percentages. Note also that the transition matrix excludes recommendations marked\nby First Call as revisions from valid to \u2018\u2018dropped.\u2019\u2019 This accounts for the di\u00a1erent number of\nobservations between Panel D and Panel A in Table I.\n\n\f1940\n\nThe Journal of Finance\n\ntarget price reports in the database are coded as one-year-ahead prices, this ratio\nmay be interpreted as the analysts\u2019 stated estimate of the \u00a2rm\u2019s annual expected\nreturn.The second measure attempts to capture whether investors react to infor-\nmation in the announced target price relative to the brokerage house\u2019s prior tar-\nget price.This measure, denoted DTP/P, is the di\u00a1erence between the current and\nprior target price issued by the same brokerage house, de\u00a3ated by stock price out-\nstanding two days prior to the announcement.7\n\nPanel A of Table II presents statistics on the two information measures as well\nas on the target price and earnings forecast revisions. We winsorize these vari-\nables at the 1st and 99th percentiles to mitigate the possible e\u00a1ect of extreme ob-\nservations. The statistics indicate that the distributions of both measures are\nright skewed. The average (median) target price is higher by 32.9 (25.5) percent\nrelative to the preannouncement stock price. As a percentage of stock price, in-\ndividual brokerage houses\u2019 target prices are 0.8 percent higher than the previous\ntarget price.8 The third column presents additional information on the change in\nthe brokerage house target price, scaling it in this case by the brokerage house\nprevious target price, DTP/TP (cid:2) 1.The average (median) percentage change in tar-\nget price is 5.3 (0) percent. Finally, in the fourth column we report summary sta-\ntistics for the earnings forecast revision measured as the change in the analyst\nforecast of earnings for the current \u00a2scal year de\u00a3ated by the stock price two days\nprior to the announcement.The mean (median) forecast revision is (cid:2) 0.41 ((cid:2) 0.03)\npercent.\n\nPanels B and C of Table II present additional information both for the level and\nchange in target prices conditioned on the associated recommendation revision.\nIn Panel B we report average target prices scaled by preannouncement stock\nprice, TP/P. In general, the magnitude of the scaled target prices is consistent\nwith the direction of the recommendation changes. For example, upgrades are\ngenerally associated with higher TP/P ratios than downgrades. Next, in Panel C\nwe report for each recommendation revision averages of DTP/TP (cid:2) 1 as well as\naverage price appreciation over the same period (since the issuance of the preced-\ning target price). It can be seen that the average DTP/TP (cid:2) 1 and the stock price\nappreciation are consistently positive for upgrades and nearly always negative\nfor downgrades. For example, an upgrade from a buy to a strong buy recommen-\ndation is associated with an average upward revision in DTP/TP (cid:2) 1 of 12.7 per-\ncent, whereas a downgrade from a buy to a hold recommendation is associated\n\n7 We have also considered additional measures. The \u00a2rst is the di\u00a1erence between a broker-\nage house\u2019s target price and the outstanding consensus target price immediately prior to the\nannouncement. Consensus target price was calculated as the average target price outstand-\ning over the previous 90 days across all brokerage houses. Other information measures are\nconstructed by scaling each of the previous target price revisions by the prior-price standard\ndeviation, measured over the 90 days preceding the event. We \u00a2nd qualitatively similar results\nin Section II with all of these information measures.\n\n8 In unreported results, we \u00a2nd that only about \u00a2ve percent of target price reports are is-\nsued below the concurrent stock price, approximately 25 percent of target price reports re-\n\u00a3ect a downward revision from brokerage houses\u2019 prior reports, and nearly 43 percent of\ntarget price reports re\u00a3ect a downward revision from the outstanding consensus target price.\n\n\fAn Empirical Analysis of Analysts\u2019 Target Prices\n\n1941\nwith a downward revision of (cid:2) 4.5 percent on average. Similarly, the average\nprice appreciation over the period preceding the announcement is also consis-\ntent with the direction of the recommendation and target price revisions. For\nthe upgrade from a buy to a strong buy recommendation, the associated stock\nprice appreciation is 5.1 percent, whereas for the downgrade from a buy to a hold\n\nTable II\n\nStatistics on Target Prices byAnalyst Stock Recommendations\n\nThis table provides descriptive statistics on the target price information measures. Panel A pro-\nvides general distributional statistics on (a) the ratio of target price to preannouncement stock\nprice (stock price outstanding two days prior to the announcement of the target price), denoted\n(TP/P), (b) the change in the individual brokerage house target price scaled by preannounce-\nment stock price, denoted (DTP/P), (c) the percentage change in the brokerage house target\nprice, denoted (DTP/TP(cid:2) 1), and (d) earnings forecast revision, computed as the di\u00a1erence in\nthe brokerage house current and prior annual earnings forecast scaled by preannouncement\nstock price. Panel B provides information on the average TP/P conditional on stock recommen-\ndation revisions. Panel C reports, for each recommendation revision, averages of DTP/TP(cid:2) 1 as\nwell as average price appreciation measured over the same period (since the issuance of the\npreceding target price). All prices and earnings are converted to the same split-adjusted basis.\n\nPanel A: Descriptive Statistics on Measures of the Information Content of Target Price\n\nTarget Price to\n\nStock Price\n\nRatio\n(TP/P)\n\nMean\nMax\n\n75th percentile\n\nMedian\n\n25th percentile\n\nMin\n\nStd. Dev.\n\nN\n\n1.329\n3.004\n1.433\n1.255\n1.146\n0.584\n0.304\n204,031\n\nChange in\nBrokerage\n\nHouse\nTarget\nPrice\n\n(DTP/P)\n\nChange in\nBrokerage\n\nHouse\nTarget\nPrice\n\n(DTP/IP(cid:2) 1)\n\n0.8%\n143.3%\n9.7%\n0.0%\n0.0%\n(cid:2) 136.0%\n78.3%\n115,720\n\n5.3%\n183.3%\n8.3%\n0.0%\n(cid:2) 0.8%\n(cid:2) 89.0%\n95.9%\n115,720\n\nPanel B: Average Target Price to Price Ratio (TP/P)\n\nTo Recommendation\n\nFrom\n\nRecommendation\n\nStrong\nBuy\n\nStrong Buy\n\nBuy\nHold\n\nSell/Strong Sell\n\nInitiated/Resumed as\n\nOverall\n\n1.40\n1.41\n1.37\n1.42\n1.43\n\n1.41\n\nBuy\n\n1.30\n1.31\n1.31\n1.36\n1.31\n\n1.31\n\nHold\n\n1.18\n1.12\n1.16\n1.11\n1.15\n\n1.16\n\nForecast\nRevision\n(cid:2) 0.41%\n35.2%\n0.16%\n(cid:2) 0.03%\n(cid:2) 0.43%\n(cid:2) 422.5%\n2.5%\n82,052\n\nSell/Strong\n\nSell\n\n1.04\n1.21\n1.01\n1.03\n1.07\n\n1.04\n\n\f1942\n\nThe Journal of Finance\n\nPanel C: Average Change in Target Price (DTP/TP (cid:2) 1) and Corresponding Price Appreciation\n\nTable II (continued )\n\nFrom\n\nRecommendation\n\nStrong\nBuy\n\nStrong Buy\n\nBuy\nHold\n\nSell/Strong Sell\nInitiated/Resumed as\n\n6.0%,\n12.7%,\n22.8%,\n20.6%,\n\n5.7%\n5.1%\n4.6%\n1.6%\n\nNA\n\nOverall\n\n6.6%,\n\n5.6%\n\n5.7%,\n\nTo Recommendation\n\nBuy\n\nHold\n\nSell/Strong\n\nSell\n\n6.4%,\n5.4%,\n16.4%,\n15.6%,\n\n2.5% (cid:2) 9.9%, (cid:2)0.2% (cid:2) 14.5%, (cid:2) 3.5%\n4.3% (cid:2) 4.5%,\n2.4% (cid:2) 6.1%,\n5.9%\n0.01% (cid:2) 7.2%, (cid:2) 2.9%\n4.4% 0.6%,\n0.5%, (cid:2) 3.5%\n2.1% 11.0%,\n0.7%\n\nNA\n\nNA\n4.2% (cid:2) 0.4%,\n\nNA\n\n0.3% (cid:2) 1.7%, (cid:2) 2.9\n\nrecommendation, stock prices appreciated on average by 2.4 percent.9 Finally, we\nnote that in the case of recommendation reiterations, the magnitude of target\nprice revisions is lower than in recommendation upgrades or downgrades.\n\nWe have also calculated statistics, as in Panels B and C, for the variation in\nearnings revisions by stock recommendation revisions (unreported). We \u00a2nd\nthat, similar to the results in these panels, earnings revisions are monotonically\nrelated to the favorableness of the recommendation change. The fact that revi-\nsions in target prices, recommendations, and earnings forecasts occur generally\nin the same direction suggests that, to some extent, these signals share much of\nthe same information content. In Section II we explore whether the information\nin each of these signals subsumes the information in any other.\n\nII. Market Reaction to Target Price Announcements\n\nA. Unconditional Informativeness of Target Prices\n\nIn this section, we examine whether the information content of target price\nannouncements is associated with abnormal returns around those announce-\nments. Speci\u00a2cally, we compute the abnormal return around each announcement\nand present average abnormal returns for portfolios ranked on the basis of the\nmagnitude of the relevant information content measure. Abnormal return is com-\nputed as the di\u00a1erence between a \u00a2rm\u2019s buy-and-hold return and the buy-and-hold\nreturn on the NYSE/AMEX/Nasdaq value-weighted market index over the period\nbeginning two days prior and ending two days subsequent to the \u00a2rm\u2019s target\nprice announcement.10 These results are reported in Figure 1.\n\nproximately three percent.\n\n9 The average contemporaneous market return for all recommendation categories is ap-\n10 Results for the period of (cid:2) 1 to \u00fe 1 days around the announcement are qualitatively simi-\nlar. Also, to avoid possible cross-correlation problems caused by identical return observations,\nwe delete all but one of identical return observations within each portfolio.\n\n\f", "Introduction": "", "Conclusion": "", "References": ""}{"Name": "Zhejiang University\nSheng Zhao Microsoft", "mail": "rayeren@zju.edu.cn,ruanyj3107@zju.edu.cn,xuta@microsoft.com,taoqin@microsoft.com,Sheng.Zhao@microsoft.com,zhaozhou@zju.edu.cn,tyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "Zhejiang University\nSheng Zhao Microsoft", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "Sheng Zhao Microsoft\nMicrosoft Research", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nWang Zhifeng Chen Patrick Nguyen Ruoming Pang Google", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Wang Zhifeng Chen Patrick Nguyen Ruoming Pang Google\nZhejiang University\nSheng Zhao Microsoft", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "Zhejiang University\nSheng Zhao Microsoft\nRon J.", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Sheng Zhao Microsoft\nSpeaker Veri\ufb01cation\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ron J.\nRen Ignacio Lopez Moreno Yonghui Wu", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\n\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\n\nEmbedding table\nProposed model\n\nVCTK Seen VCTK Unseen\n4.43 \u00b1 0.05\n4.49 \u00b1 0.05\n4.12 \u00b1 0.06\n4.20 \u00b1 0.06\n4.07 \u00b1 0.06\n\nN/A\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\n4.12 \u00b1 0.05\n\nN/A\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nGround truth\nGround truth\nGround truth\n\nSpeaker Set\nSame speaker\nSame gender\n\nDifferent gender\n\nEmbedding table\nProposed model\nProposed model\n\nSeen\nSeen\nUnseen\n\nVCTK\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n3.28 \u00b1 0.07\n\nLibriSpeech\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nVCTK\n\nLibriSpeech\n\nTesting Set\nLibriSpeech\n\nVCTK\n\nNaturalness\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\nSimilarity\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\n\nVCTK\n\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nLS-Clean\nLS-Other\n\nLS-Other + VC\n\nLS-Other + VC + VC2\n\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\nEmbedding Dim Naturalness\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n64\n64\n256\n256\n256\n\nSimilarity\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\nSV-EER\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\nSynthesizer train set\n\nSpeaker Encoder train set\n\nCosine similarity\n\n0.222\n0.245\n\nSV-EER Naturalness MOS\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fAcknowledgements\n\nThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "Ren Ignacio Lopez Moreno Yonghui Wu\nZhejiang University\nSheng Zhao Microsoft", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "Zhejiang University\nSheng Zhao Microsoft\nMicrosoft Research", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\nprocess.\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho,D, \u03b1),\n\n(1)\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\ffollowing [13].\n\nIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\nduration extractor di =(cid:80)S\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n3.88 \u00b1 0.09\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\nLatency (s)\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\nSpeedup\n\n269.40\u00d7\n\n38.30\u00d7\n\n/\n\n/\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\nTransformer TTS\nFastSpeech\n\nRepeats\n\nSkips Error Sentences Error Rate\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\nFastSpeech\nFastSpeech without 1D convolution in FFT block\nFastSpeech without sequence-level knowledge distillation\nFastSpeech without weight initialization from teacher model\n\nCMOS\n\n0\n\n-0.113\n-0.325\n-0.061\n\nTable 4: CMOS comparison in the ablation studies.\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}{"Name": "", "mail": "{jiaye,ngyuzh,ronw}@google.com", "Abstract": "\n\nWe describe a neural network-based system for text-to-speech (TTS) synthesis that\nis able to generate speech audio in the voice of different speakers, including those\nunseen during training. Our system consists of three independently trained compo-\nnents: (1) a speaker encoder network, trained on a speaker veri\ufb01cation task using an\nindependent dataset of noisy speech without transcripts from thousands of speakers,\nto generate a \ufb01xed-dimensional embedding vector from only seconds of reference\nspeech from a target speaker; (2) a sequence-to-sequence synthesis network based\non Tacotron 2 that generates a mel spectrogram from text, conditioned on the\nspeaker embedding; (3) an auto-regressive WaveNet-based vocoder network that\nconverts the mel spectrogram into time domain waveform samples. We demonstrate\nthat the proposed model is able to transfer the knowledge of speaker variability\nlearned by the discriminatively-trained speaker encoder to the multispeaker TTS\ntask, and is able to synthesize natural speech from speakers unseen during training.\nWe quantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we show\nthat randomly sampled speaker embeddings can be used to synthesize speech in\nthe voice of novel speakers dissimilar from those used in training, indicating that\nthe model has learned a high quality speaker representation.\n\n1\n", "Introduction": "\n\nThe goal of this work is to build a TTS system which can generate natural speech for a variety of\nspeakers in a data ef\ufb01cient manner. We speci\ufb01cally address a zero-shot learning setting, where a\nfew seconds of untranscribed reference audio from a target speaker is used to synthesize new speech\nin that speaker\u2019s voice, without updating any model parameters. Such systems have accessibility\napplications, such as restoring the ability to communicate naturally to users who have lost their\nvoice and are therefore unable to provide many new training examples. They could also enable\nnew applications, such as transferring a voice across languages for more natural speech-to-speech\ntranslation, or generating realistic speech from text in low resource settings. However, it is also\nimportant to note the potential for misuse of this technology, for example impersonating someone\u2019s\nvoice without their consent. In order to address safety concerns consistent with principles such as [1],\nwe verify that voices generated by the proposed model can easily be distinguished from real voices.\n\nSynthesizing natural speech requires training on a large number of high quality speech-transcript\npairs, and supporting many speakers usually uses tens of minutes of training data per speaker [8].\nRecording a large amount of high quality data for many speakers is impractical. Our approach is to\ndecouple speaker modeling from speech synthesis by independently training a speaker-discriminative\nembedding network that captures the space of speaker characteristics and training a high quality TTS\n\n\u2217Equal contribution.\n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\n\n\fmodel on a smaller dataset conditioned on the representation learned by the \ufb01rst network. Decoupling\nthe networks enables them to be trained on independent data, which reduces the need to obtain high\nquality multispeaker training data. We train the speaker embedding network on a speaker veri\ufb01cation\ntask to determine if two different utterances were spoken by the same speaker. In contrast to the\nsubsequent TTS model, this network is trained on untranscribed speech containing reverberation and\nbackground noise from a large number of speakers.\n\nWe demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and\ndisjoint sets of speakers and still generalize well. We train the synthesis network on 1.2K speakers\nand show that training the encoder on a much larger set of 18K speakers improves adaptation quality,\nand further enables synthesis of completely novel speakers by sampling from the embedding prior.\n\nThere has been signi\ufb01cant interest in end-to-end training of TTS models, which are trained directly\nfrom text-audio pairs, without depending on hand crafted intermediate representations [17, 23].\nTacotron 2 [15] used WaveNet [19] as a vocoder to invert spectrograms generated by an encoder-\ndecoder architecture with attention [3], obtaining naturalness approaching that of human speech by\ncombining Tacotron\u2019s [23] prosody with WaveNet\u2019s audio quality. It only supported a single speaker.\n\nGibiansky et al. [8] introduced a multispeaker variation of Tacotron which learned low-dimensional\nspeaker embedding for each training speaker. Deep Voice 3 [13] proposed a fully convolutional\nencoder-decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech [12].\n\nThese systems learn a \ufb01xed set of speaker embeddings and therefore only support synthesis of voices\nseen during training. In contrast, VoiceLoop [18] proposed a novel architecture based on a \ufb01xed\nsize memory buffer which can generate speech from voices unseen during training. Obtaining good\nresults required tens of minutes of enrollment speech and transcripts for a new speaker.\n\nRecent extensions have enabled few-shot speaker adaptation where only a few seconds of speech\nper speaker (without transcripts) can be used to generate new speech in that speaker\u2019s voice. [2]\nextends Deep Voice 3, comparing a speaker adaptation method similar to [18] where the model\nparameters (including speaker embedding) are \ufb01ne-tuned on a small amount of adaptation data to a\nspeaker encoding method which uses a neural network to predict speaker embedding directly from a\nspectrogram. The latter approach is signi\ufb01cantly more data ef\ufb01cient, obtaining higher naturalness\nusing small amounts of adaptation data, in as few as one or two utterances. It is also signi\ufb01cantly\nmore computationally ef\ufb01cient since it does not require hundreds of backpropagation iterations.\n\nNachmani et al. [10] similarly extended VoiceLoop to utilize a target speaker encoding network to\npredict a speaker embedding. This network is trained jointly with the synthesis network using a\ncontrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are\ncloser than embeddings computed from different speakers. In addition, a cycle-consistency loss is\nused to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance.\n\nA similar spectrogram encoder network, trained without a triplet loss, was shown to work for\ntransferring target prosody to synthesized speech [16]. In this paper we demonstrate that training a\nsimilar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics.\nOur work is most similar to the speaker encoding models in [2, 10], except that we utilize a network\nindependently-trained for a speaker veri\ufb01cation task on a large dataset of untranscribed audio from tens\nof thousands of speakers, using a state-of-the-art generalized end-to-end loss [22]. [10] incorporated\na similar speaker-discriminative representation into their model, however all components were trained\njointly. In contrast, we explore transfer learning from a pre-trained speaker veri\ufb01cation model.\n\nDoddipatla et al. [7] used a similar transfer learning con\ufb01guration where a speaker embedding\ncomputed from a pre-trained speaker classi\ufb01er was used to condition a TTS system. In this paper we\nutilize an end-to-end synthesis network which does not rely on intermediate linguistic features, and a\nsubstantially different speaker embedding network which is not limited to a closed set of speakers.\nFurthermore, we analyze how quality varies with the number of speakers in the training set, and \ufb01nd\nthat zero-shot transfer requires training on thousands of speakers, many more than were used in [7].\n\n2 Multispeaker speech synthesis model\n\nOur system is composed of three independently trained neural networks, illustrated in Figure 1: (1) a\nrecurrent speaker encoder, based on [22], which computes a \ufb01xed dimensional vector from a speech\n\n2\n\n\fSpeaker\nEncoder\n\nSynthesizer\n\nspeaker\nembedding\n\nspeaker\nreference\nwaveform\n\ngrapheme or\nphoneme\nsequence\n\nEncoder\n\nconcat\n\nAttention\n\nDecoder\n\nVocoder\n\nwaveform\n\nFigure 1: Model overview. Each of the three components are trained independently.\n\nlog-mel\nspectrogram\n\nsignal, (2) a sequence-to-sequence synthesizer, based on [15], which predicts a mel spectrogram from\na sequence of grapheme or phoneme inputs, conditioned on the speaker embedding vector, and (3) an\nautoregressive WaveNet [19] vocoder, which converts the spectrogram into time domain waveforms.1\n\n2.1 Speaker encoder\n\nThe speaker encoder is used to condition the synthesis network on a reference speech signal from the\ndesired target speaker. Critical to good generalization is the use of a representation which captures the\ncharacteristics of different speakers, and the ability to identify these characteristics using only a short\nadaptation signal, independent of its phonetic content and background noise. These requirements are\nsatis\ufb01ed using a speaker-discriminative model trained on a text-independent speaker veri\ufb01cation task.\n\nWe follow [22], which proposed a highly scalable and accurate neural network framework for speaker\nveri\ufb01cation. The network maps a sequence of log-mel spectrogram frames computed from a speech\nutterance of arbitrary length, to a \ufb01xed-dimensional embedding vector, known as d-vector [20, 9]. The\nnetwork is trained to optimize a generalized end-to-end speaker veri\ufb01cation loss, so that embeddings\nof utterances from the same speaker have high cosine similarity, while those of utterances from\ndifferent speakers are far apart in the embedding space. The training dataset consists of speech audio\nexamples segmented into 1.6 seconds and associated speaker identity labels; no transcripts are used.\n\nInput 40-channel log-mel spectrograms are passed to a network consisting of a stack of 3 LSTM\nlayers of 768 cells, each followed by a projection to 256 dimensions. The \ufb01nal embedding is created\nby L2-normalizing the output of the top layer at the \ufb01nal frame. During inference, an arbitrary length\nutterance is broken into 800ms windows, overlapped by 50%. The network is run independently on\neach window, and the outputs are averaged and normalized to create the \ufb01nal utterance embedding.\n\nAlthough the network is not optimized directly to learn a representation which captures speaker\ncharacteristics relevant to synthesis, we \ufb01nd that training on a speaker discrimination task leads to an\nembedding which is directly suitable for conditioning the synthesis network on speaker identity.\n\n2.2 Synthesizer\n\nWe extend the recurrent sequence-to-sequence with attention Tacotron 2 architecture [15] to support\nmultiple speakers following a scheme similar to [8]. An embedding vector for the target speaker is\nconcatenated with the synthesizer encoder output at each time step. In contrast to [8], we \ufb01nd that\nsimply passing embeddings to the attention layer, as in Figure 1, converges across different speakers.\n\nWe compare two variants of this model, one which computes the embedding using the speaker\nencoder, and a baseline which optimizes a \ufb01xed embedding for each speaker in the training set,\nessentially learning a lookup table of speaker embeddings similar to [8, 13].\n\nThe synthesizer is trained on pairs of text transcript and target audio. At the input, we map the text to\na sequence of phonemes, which leads to faster convergence and improved pronunciation of rare words\nand proper nouns. The network is trained in a transfer learning con\ufb01guration, using a pretrained\nspeaker encoder (whose parameters are frozen) to extract a speaker embedding from the target audio,\ni.e. the speaker reference signal is the same as the target speech during training. No explicit speaker\nidenti\ufb01er labels are used during training.\n\nTarget spectrogram features are computed from 50ms windows computed with a 12.5ms step, passed\nthrough an 80-channel mel-scale \ufb01lterbank followed by log dynamic range compression. We extend\n[15] by augmenting the L2 loss on the predicted spectrogram with an additional L1 loss. In practice,\n\n1See https://google.github.io/tacotron/publications/speaker_adaptation for samples.\n\n3\n\n\fFigure 2: Example synthesis of a sentence in different voices using the proposed system. Mel\nspectrograms are visualized for reference utterances used to generate speaker embeddings (left), and\nthe corresponding synthesizer outputs (right). The text-to-spectrogram alignment is shown in red.\nThree speakers held out of the train sets are used: one male (top) and two female (center and bottom).\n\nwe found this combined loss to be more robust on noisy training data. In contrast to [10], we don\u2019t\nintroduce additional loss terms based on the speaker embedding.\n\n2.3 Neural vocoder\n\nWe use the sample-by-sample autoregressive WaveNet [19] as a vocoder to invert synthesized mel\nspectrograms emitted by the synthesis network into time-domain waveforms. The architecture is the\nsame as that described in [15], composed of 30 dilated convolution layers. The network is not directly\nconditioned on the output of the speaker encoder. The mel spectrogram predicted by the synthesizer\nnetwork captures all of the relevant detail needed for high quality synthesis of a variety of voices,\nallowing a multispeaker vocoder to be constructed by simply training on data from many speakers.\n\n2.4\n\nInference and zero-shot speaker adaptation\n\nDuring inference the model is conditioned using arbitrary untranscribed speech audio, which does\nnot need to match the text to be synthesized. Since the speaker characteristics to use for synthesis are\ninferred from audio, it can be conditioned on audio from speakers that are outside the training set. In\npractice we \ufb01nd that using a single audio clip of a few seconds duration is suf\ufb01cient to synthesize\nnew speech with the corresponding speaker characteristics, representing zero-shot adaptation to novel\nspeakers. In Section 3 we evaluate how well this process generalizes to previously unseen speakers.\n\nAn example of the inference process is visualized in Figure 2, which shows spectrograms synthesized\nusing several different 5 second speaker reference utterances. Compared to those of the female\n(center and bottom) speakers, the synthesized male (top) speaker spectrogram has noticeably lower\nfundamental frequency, visible in the denser harmonic spacing (horizontal stripes) in low frequencies,\nas well as formants, visible in the mid-frequency peaks present during vowel sounds such as the \u2018i\u2019 at\n0.3 seconds \u2013 the top male F2 is in mel channel 35, whereas the F2 of the middle speaker appears\ncloser to channel 40. Similar differences are also visible in sibilant sounds, e.g. the \u2018s\u2019 at 0.4 seconds\ncontains more energy in lower frequencies in the male voice than in the female voices. Finally, the\ncharacteristic speaking rate is also captured to some extent by the speaker embedding, as can be seen\n\n4\n\n0.00.51.01.52.001020304050607080Mel channelthisisabigredappleSynthesized mel spectrogram\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 7021Mel channelSpeaker reference utterance\"and all his brothers and sisters stood round and listenedwith their mouths open\"4321012340.00.51.01.52.001020304050607080Mel channelthisisabigredapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.501020304050607080Speaker 3570Mel channel\"but it will appear in the sequel that this exception ismuch more obvious than substantial\"4321012340.00.51.01.52.0Time (sec)01020304050607080Mel channelthisisabigredaapple\"this is a big red apple\"630360.00.51.01.52.02.53.03.54.04.5Time (sec)01020304050607080Speaker 4992Mel channel\"and the firebugs can't think o the right name somethinglike cendenaries\"432101234\fTable 1: Speech naturalness Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nVCTK Seen VCTK Unseen\n\nLibriSpeech Seen\n\nLibriSpeech Unseen\n\nGround truth\nEmbedding table\nProposed model\n\n4.43 \u00b1 0.05\n4.12 \u00b1 0.06\n4.07 \u00b1 0.06\n\n4.49 \u00b1 0.05\nN/A\n4.20 \u00b1 0.06\n\n4.49 \u00b1 0.05\n3.90 \u00b1 0.06\n3.89 \u00b1 0.06\n\n4.42 \u00b1 0.07\nN/A\n4.12 \u00b1 0.05\n\nby the longer signal duration in the bottom row compared to the top two. Similar observations can be\nmade about the corresponding reference utterance spectrograms in the right column.\n\n3 Experiments\n\nWe used two public datasets for training the speech synthesis and vocoder networks. VCTK [21]\ncontains 44 hours of clean speech from 109 speakers, the majority of which have British accents. We\ndownsampled the audio to 24 kHz, trimmed leading and trailing silence (reducing the median duration\nfrom 3.3 seconds to 1.8 seconds), and split into three subsets: train, validation (containing the same\nspeakers as the train set) and test (containing 11 speakers held out from the train and validation sets).\n\nLibriSpeech [12] consists of the union of the two \u201cclean\u201d training sets, comprising 436 hours of\nspeech from 1,172 speakers, sampled at 16 kHz. The majority of speech is US English, however since\nit is sourced from audio books, the tone and style of speech can differ signi\ufb01cantly between utterances\nfrom the same speaker. We resegmented the data into shorter utterances by force aligning the audio to\nthe transcript using an ASR model and breaking segments on silence, reducing the median duration\nfrom 14 to 5 seconds. As in the original dataset, there is no punctuation in transcripts. The speaker\nsets are completely disjoint among the train, validation, and test sets.\n\nMany recordings in the LibriSpeech clean corpus contain noticeable environmental and stationary\nbackground noise. We preprocessed the target spectrogram using a simple spectral subtraction [4]\ndenoising procedure, where the background noise spectrum of an utterance was estimated as the 10th\npercentile of the energy in each frequency band across the full signal. This process was only used on\nthe synthesis target; the original noisy speech was passed to the speaker encoder.\n\nWe trained separate synthesis and vocoder networks for each of these two corpora. Throughout this\nsection, we used synthesis networks trained on phoneme inputs, in order to control for pronunciation\nin subjective evaluations. For the VCTK dataset, whose audio is quite clean, we found that the\nvocoder trained on ground truth mel spectrograms worked well. However for LibriSpeech, which\nis noisier, we found it necessary to train the vocoder on spectrograms predicted by the synthesizer\nnetwork. No denoising was performed on the target waveform for vocoder training.\n\nThe speaker encoder was trained on a proprietary voice search corpus containing 36M utterances with\nmedian duration of 3.9 seconds from 18K English speakers in the United States. This dataset is not\ntranscribed, but contains anonymized speaker identities. It is never used to train synthesis networks.\n\nWe primarily rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective\nlistening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale [14], with\nrating scores from 1 to 5 in 0.5 point increments. We use this framework to evaluate synthesized\nspeech along two dimensions: its naturalness and similarity to real speech from the target speaker.\n\n3.1 Speech naturalness\n\nWe compared the naturalness of synthesized speech using synthesizers and vocoders trained on VCTK\nand LibriSpeech. We constructed an evaluation set of 100 phrases which do not appear in any training\nsets, and evaluated two sets of speakers for each model: one composed of speakers included in the\ntrain set (Seen), and another composed of those that were held out (Unseen). We used 11 seen and\nunseen speakers for VCTK and 10 seen and unseen speakers for LibriSpeech (Appendix D). For each\nspeaker, we randomly chose one utterance with duration of about 5 seconds to use to compute the\nspeaker embedding (see Appendix C). Each phrase was synthesized for each speaker, for a total of\nabout 1,000 synthesized utterances per evaluation. Each sample was rated by a single rater, and each\nevaluation was conducted independently: the outputs of different models were not compared directly.\n\n5\n\n\fTable 2: Speaker similarity Mean Opinion Score (MOS) with 95% con\ufb01dence intervals.\n\nSystem\n\nSpeaker Set\n\nVCTK\n\nLibriSpeech\n\nGround truth\nGround truth\nGround truth\n\nSame speaker\nSame gender\nDifferent gender\n\n4.67 \u00b1 0.04\n2.25 \u00b1 0.07\n1.15 \u00b1 0.04\n\n4.33 \u00b1 0.08\n1.83 \u00b1 0.07\n1.04 \u00b1 0.03\n\nEmbedding table\nProposed model\n\nSeen\nSeen\n\n4.17 \u00b1 0.06\n4.22 \u00b1 0.06\n\n3.70 \u00b1 0.08\n3.28 \u00b1 0.08\n\nProposed model\n\nUnseen\n\n3.28 \u00b1 0.07\n\n3.03 \u00b1 0.09\n\nResults are shown in Table 1, comparing the proposed model to baseline multispeaker models\nthat utilize a lookup table of speaker embeddings similar to [8, 13], but otherwise have identical\narchitectures to the proposed synthesizer network. The proposed model achieved about 4.0 MOS in\nall datasets, with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech\nmodel when evaluated on seen speakers. This is the consequence of two drawbacks of the LibriSpeech\ndataset: (i) the lack of punctuation in transcripts, which makes it dif\ufb01cult for the model to learn to\npause naturally, and (ii) the higher level of background noise compared to VCTK, some of which the\nsynthesizer has learned to reproduce, despite denoising the training targets as described above.\n\nMost importantly, the audio generated by our model for unseen speakers is deemed to be at least as\nnatural as that generated for seen speakers. Surprisingly, the MOS on unseen speakers is higher than\nthat of seen speakers, by as much as 0.2 points on LibriSpeech. This is a consequence of the randomly\nselected reference utterance for each speaker, which sometimes contains uneven and non-neutral\nprosody. In informal listening tests we found that the prosody of the synthesized speech sometimes\nmimics that of the reference, similar to [16]. This effect is larger on LibriSpeech, which contains\nmore varied prosody. This suggests that additional care must be taken to disentangle speaker identity\nfrom prosody within the synthesis network, perhaps by integrating a prosody encoder as in [16, 24],\nor by training on randomly paired reference and target utterances from the same speaker.\n\n3.2 Speaker similarity\n\nTo evaluate how well the synthesized speech matches that from the target speaker, we paired each\nsynthesized utterance with a randomly selected ground truth utterance from the same speaker. Each\npair is rated by one rater with the following instructions: \u201cYou should not judge the content, grammar,\nor audio quality of the sentences; instead, just focus on the similarity of the speakers to one another.\u201d\n\nResults are shown in Table 2. The scores for the VCTK model tend to be higher than those for\nLibriSpeech, re\ufb02ecting the cleaner nature of the dataset. This is also evident in the higher ground truth\nbaselines on VCTK. For seen speakers on VCTK, the proposed model performs about as well as the\nbaseline which uses an embedding lookup table for speaker conditioning. However, on LibriSpeech,\nthe proposed model obtained a lower similarity MOS than the baseline, which is likely due to the\nwider degree of within-speaker variation (Appendix B), and background noise level in the dataset.\n\nOn unseen speakers, the proposed model obtains lower similarity between ground truth and synthe-\nsized speech. On VCTK, the similarity score of 3.28 is between \u201cmoderately similar\u201d and \u201cvery\nsimilar\u201d on the evaluation scale. Informally, it is clear that the proposed model is able to transfer the\nbroad strokes of the speaker characteristics for unseen speakers, clearly re\ufb02ecting the correct gender,\npitch, and formant ranges (as also visualized in Figure 2). But the signi\ufb01cantly reduced similarity\nscores on unseen speakers suggests that some nuances, e.g. related to characteristic prosody, are lost.\n\nThe speaker encoder is trained only on North American accented speech. As a result, accent mismatch\nconstrains our performance on speaker similarity on VCTK since the rater instructions did not specify\nhow to judge accents, so raters may consider a pair to be from different speakers if the accents do not\nmatch. Indeed, examination of rater comments shows that our model sometimes produced a different\naccent than the ground truth, which led to lower scores. However, a few raters commented that the\ntone and in\ufb02ection of the voices sounded very similar despite differences in accent.\n\nAs an initial evaluation of the ability to generalize to out of domain speakers, we used synthesizers\ntrained on VCTK and LibriSpeech to synthesize speakers from the other dataset. We only varied the\ntrain set of the synthesizer and vocoder networks; both models used an identical speaker encoder. As\n\n6\n\n\fTable 3: Cross-dataset evaluation on naturalness and speaker similarity for unseen speakers.\n\nSynthesizer Training Set\n\nTesting Set\n\nNaturalness\n\nSimilarity\n\nVCTK\nLibriSpeech\n\nLibriSpeech\nVCTK\n\n4.28 \u00b1 0.05\n4.01 \u00b1 0.06\n\n1.82 \u00b1 0.08\n2.77 \u00b1 0.08\n\nTable 4: Speaker veri\ufb01cation EERs of different synthesizers on unseen speakers.\n\nSynthesizer Training Set\n\nTraining Speakers\n\nSV-EER on VCTK SV-EER on LibriSpeech\n\nGround truth\nVCTK\nLibriSpeech\n\n\u2013\n98\n1.2K\n\n1.53%\n10.46%\n6.26%\n\n0.93%\n29.19%\n5.08%\n\nshown in Table 3, the models were able to generate speech with the same degree of naturalness as\non unseen, but in-domain, speakers shown in Table 1. However, the LibriSpeech model synthesized\nVCTK speakers with signi\ufb01cantly higher speaker similarity than the VCTK model is able to synthesize\nLibriSpeech speakers. The better generalization of the LibriSpeech model suggests that training the\nsynthesizer on only 100 speakers is insuf\ufb01cient to enable high quality speaker transfer.\n\n3.3 Speaker veri\ufb01cation\n\nAs an objective metric of the degree of speaker similarity between synthesized and ground truth audio\nfor unseen speakers, we evaluated the ability of a limited speaker veri\ufb01cation system to distinguish\nsynthetic from real speech. We trained a new eval-only speaker encoder with the same network\ntopology as Section 2.1, but using a different training set of 28M utterances from 113K speakers.\nUsing a different model for evaluation ensured that metrics were not only valid on a speci\ufb01c speaker\nembedding space. We enroll the voices of 21 real speakers: 11 speakers from VCTK, and 10 from\nLibriSpeech, and score synthesized waveforms against the set of enrolled speakers. All enrollment\nand veri\ufb01cation speakers are unseen during synthesizer training. Speaker veri\ufb01cation equal error rates\n(SV-EERs) are estimated by pairing each test utterance with each enrollment speaker. We synthesized\n100 test utterances for each speaker, so 21,000 or 23,100 trials were performed for each evaluation.\n\nAs shown in Table 4, as long as the synthesizer was trained on a suf\ufb01ciently large set of speakers,\ni.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices. The\nLibriSpeech synthesizer obtains similar EERs of 5-6% using reference speakers from both datasets,\nwhereas the one trained on VCTK performs much worse, especially on out-of-domain LibriSpeech\nspeakers. These results are consistent with the subjective evaluation in Table 3.\n\nTo measure the dif\ufb01culty of discriminating between real and synthetic speech for the same speaker, we\nperformed an additional evaluation with an expanded set of enrolled speakers including 10 synthetic\nversions of the 10 real LibriSpeech speakers. On this 20 voice discrimination task we obtain an\nEER of 2.86%, demonstrating that, while the synthetic speech tends to be close to the target speaker\n(cosine similarity > 0.6, and as in Table 4), it is nearly always even closer to other synthetic utterances\nfor the same speaker (similarity > 0.7). From this we can conclude that the proposed model can\ngenerate speech that resembles the target speaker, but not well enough to be confusable with a real\nspeaker.\n\n3.4 Speaker embedding space\n\nVisualizing the speaker embedding space further contextualizes the quantitive results described in\nSection 3.2 and 3.3. As shown in Figure 3, different speakers are well separated from each other in\nthe speaker embedding space. The PCA visualization (left) shows that synthesized utterances tend\nto lie very close to real speech from the same speaker in the embedding space. However, synthetic\nutterances are still easily distinguishable from the real human speech as demonstrated by the t-SNE\nvisualization (right) where utterances from each synthetic speaker form a distinct cluster adjacent to a\ncluster of real utterances from the corresponding speaker.\n\n7\n\n\fFigure 3: Visualization of speaker embeddings extracted from LibriSpeech utterances. Each color\ncorresponds to a different speaker. Real and synthetic utterances appear nearby when they are from\nthe same speaker, however real and synthetic utterances consistently form distinct clusters.\n\nTable 5: Performance using speaker encoders (SEs) trained on different datasets. Synthesizers are all\ntrained on LibriSpeech Clean and evaluated on held out speakers. LS: LibriSpeech, VC: VoxCeleb.\n\nSE Training Set\n\nSpeakers\n\nEmbedding Dim Naturalness\n\nSimilarity\n\nSV-EER\n\nLS-Clean\nLS-Other\nLS-Other + VC\nLS-Other + VC + VC2\nInternal\n\n1.2K\n1.2K\n2.4K\n8.4K\n18K\n\n64\n64\n256\n256\n256\n\n3.73 \u00b1 0.06\n3.60 \u00b1 0.06\n3.83 \u00b1 0.06\n3.82 \u00b1 0.06\n4.12 \u00b1 0.05\n\n2.23 \u00b1 0.08\n2.27 \u00b1 0.09\n2.43 \u00b1 0.09\n2.54 \u00b1 0.09\n3.03 \u00b1 0.09\n\n16.60%\n15.32%\n11.95%\n10.14%\n5.08%\n\nSpeakers appear to be well separated by gender in both the PCA and t-SNE visualizations, with\nall female speakers appearing on the left, and all male speakers appearing on the right. This is an\nindication that the speaker encoder has learned a reasonable representation of speaker space.\n\n3.5 Number of speaker encoder training speakers\n\nIt is likely that the ability of the proposed model to generalize well across a wide variety of speakers\nis based on the quality of the representation learned by the speaker encoder. We therefore explored\nthe effect of the speaker encoder training set on synthesis quality. We made use of three additional\ntraining sets: (1) LibriSpeech Other, which contains 461 hours of speech from a set of 1,166 speakers\ndisjoint from those in the clean subsets, (2) VoxCeleb [11], and (3) VoxCeleb2 [6] which contain\n139K utterances from 1,211 speakers, and 1.09M utterances from 5,994 speakers, respectively.\n\nTable 5 compares the performance of the proposed model as a function of the number of speakers\nused to train the speaker encoder. This measures the importance of speaker diversity when training\nthe speaker encoder. To avoid over\ufb01tting, the speaker encoders trained on small datasets (top two\nrows) use a smaller network architecture (256-dim LSTM cells with 64-dim projections) and output\n64 dimensional speaker embeddings.\n\nWe \ufb01rst evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which\ncontain a similar number of speakers. In Clean, the speaker encoder and synthesizer are trained on\nthe same data, a baseline similar to the non-\ufb01ne tuned speaker encoder from [2], except that it is\ntrained discriminatively as in [10]. This matched condition gives a slightly better naturalness and a\nsimilar similarity score. As the number of training speakers increases, both naturalness and similarity\nimprove signi\ufb01cantly. The objective EER results also improve alongside the subjective evaluations.\n\nThese results have an important implication for multispeaker TTS training. The data requirement for\nthe speaker encoder is much cheaper than full TTS training since no transcripts are necessary, and the\naudio quality can be lower than for TTS training. We have shown that it is possible to synthesize very\n\n8\n\nFemaleMalePCAHumanSynthesizedFemaleMalet-SNEHumanSynthesized\fTable 6: Speech from \ufb01ctitious speakers compared to their nearest neighbors in the train sets.\nSynthesizer was trained on LS Clean. Speaker Encoder was trained on LS-Other + VC + VC2.\n\nNearest neighbors in\n\nCosine similarity\n\nSV-EER Naturalness MOS\n\nSynthesizer train set\nSpeaker Encoder train set\n\n0.222\n0.245\n\n56.77%\n38.54%\n\n3.65 \u00b1 0.06\n\nnatural TTS by combining a speaker encoder network trained on large amounts of untranscribed data\nwith a TTS network trained on a smaller set of high quality data.\n\n3.6 Fictitious speakers\n\nBypassing the speaker encoder network and conditioning the synthesizer on random points in the\nspeaker embedding space results in speech from \ufb01ctitious speakers which are not present in the train\nor test sets of either the synthesizer or the speaker encoder. This is demonstrated in Table 6, which\ncompares 10 such speakers, generated from uniformly sampled points on the surface of the unit\nhypersphere, to their nearest neighbors in the training sets of the component networks. SV-EERs\nare computed using the same setup as Section 3.3 after enrolling voices of the 10 nearest neighbors.\nEven though these speakers are totally \ufb01ctitious, the synthesizer and the vocoder are able to generate\naudio as natural as for seen or unseen real speakers. The low cosine similarity to the nearest neighbor\ntraining utterances and very high EER indicate that they are indeed distinct from the training speakers.\n", "Conclusion": "\n\nWe present a neural network-based system for multispeaker TTS synthesis. The system combines an\nindependently trained speaker encoder network with a sequence-to-sequence TTS synthesis network\nand neural vocoder based on Tacotron 2. By leveraging the knowledge learned by the discriminative\nspeaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen\nduring training, but also for speakers never seen before. Through evaluations based on a speaker\nveri\ufb01cation system as well as subjective listening tests, we demonstrated that the synthesized speech\nis reasonably similar to real speech from the target speakers, even on such unseen speakers.\n\nWe ran experiments to analyze the impact of the amount of data used to train the different components,\nand found that, given suf\ufb01cient speaker diversity in the synthesizer training set, speaker transfer\nquality could be signi\ufb01cantly improved by increasing the amount of speaker encoder training data.\n\nTransfer learning is critical to achieving these results. By separating the training of the speaker\nencoder and the synthesizer, the system signi\ufb01cantly lowers the requirements for multispeaker\nTTS training data. It requires neither speaker identity labels for the synthesizer training data, nor\nhigh quality clean speech or transcripts for the speaker encoder training data. In addition, training\nthe components independently signi\ufb01cantly simpli\ufb01es the training con\ufb01guration of the synthesizer\nnetwork compared to [10] since it does not require additional triplet or contrastive losses. However,\nmodeling speaker variation using a low dimensional vector limits the ability to leverage large amounts\nof reference speech. Improving speaker similarity given more than a few seconds of reference speech\nrequires a model adaptation approach as in [2], and more recently in [5].\n\nFinally, we demonstrate that the model is able to generate realistic speech from \ufb01ctitious speakers\nthat are dissimilar from the training set, implying that the model has learned to utilize a realistic\nrepresentation of the space of speaker variation.\n\nThe proposed model does not attain human-level naturalness, despite the use of a WaveNet vocoder\n(along with its very high inference cost), in contrast to the single speaker results from [15]. This\nis a consequence of the additional dif\ufb01culty of generating speech for a variety of speakers given\nsigni\ufb01cantly less data per speaker, as well as the use of datasets with lower data quality. An additional\nlimitation lies in the model\u2019s inability to transfer accents. Given suf\ufb01cient training data, this could be\naddressed by conditioning the synthesizer on independent speaker and accent embeddings. Finally,\nwe note that the model is also not able to completely isolate the speaker voice from the prosody of\nthe reference audio, a similar trend to that observed in [16].\n\n9\n\n\fThe authors thank Heiga Zen, Yuxuan Wang, Samy Bengio, the Google AI Perception team, and the\nGoogle TTS and DeepMind Research teams for their helpful discussions and feedback.\n\nAcknowledgements\n", "References": "\n\n[1] Arti\ufb01cial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.\n\n[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloningwith a few samples. arXiv preprint arXiv:1802.06006, 2018.\n\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In Proceedings of ICLR, 2015.\n\n[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-tions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.\n\n[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, QuanWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample ef\ufb01cient adaptive text-to-speech.arXiv preprint arXiv:1809.10460, 2018.\n\n[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition.In Interspeech, pages 1086\u20131090, 2018.\n\n[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnn-based speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.\n\n[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, JonathanRaiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V.Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advancesin Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.\n\n[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependentspeaker veri\ufb01cation. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-tional Conference on, pages 5115\u20135119. IEEE, 2016.\n\n[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based ona short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.\n\n[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speakeridenti\ufb01cation dataset. arXiv preprint arXiv:1706.08612, 2017.\n\n[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: anASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing(ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.\n\n[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc.International Conference on Learning Representations (ICLR), 2018.\n\n[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. InternationalTelecommunication Union, Geneva, 1996.\n\n[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, ZonghengYang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, YannisAgiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on melspectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, andSignal Processing (ICASSP), 2018.\n\n[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J.Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressivespeech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.10\f"}{"Name": "", "mail": "rayeren@zju.edu.cn\nruanyj3107@zju.edu.cn\nxuta@microsoft.com\ntaoqin@microsoft.com\nSheng.Zhao@microsoft.com\nzhaozhou@zju.edu.cn\ntyliu@microsoft.com", "Abstract": "\n\nNeural network based end-to-end text to speech (TTS) has signi\ufb01cantly improved\nthe quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually\n\ufb01rst generate mel-spectrogram from text, and then synthesize speech from the\nmel-spectrogram using vocoder such as WaveNet. Compared with traditional\nconcatenative and statistical parametric approaches, neural network based end-\nto-end models suffer from slow inference speed, and the synthesized speech is\nusually not robust (i.e., some words are skipped or repeated) and lack of con-\ntrollability (voice speed or prosody control). In this work, we propose a novel\nfeed-forward network based on Transformer to generate mel-spectrogram in paral-\nlel for TTS. Speci\ufb01cally, we extract attention alignments from an encoder-decoder\nbased teacher model for phoneme duration prediction, which is used by a length\nregulator to expand the source phoneme sequence to match the length of the target\nmel-spectrogram sequence for parallel mel-spectrogram generation. Experiments\non the LJSpeech dataset show that our parallel model matches autoregressive mod-\nels in terms of speech quality, nearly eliminates the problem of word skipping and\nrepeating in particularly hard cases, and can adjust voice speed smoothly. Most\nimportantly, compared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech. We will release the code on Github.3\n\n1\n", "Introduction": "\n\nText to speech (TTS) has attracted a lot of attention in recent years due to the advance in deep\nlearning. Deep neural network based systems have become more and more popular for TTS, such\nas Tacotron [24], Tacotron 2 [20], Deep Voice 3 [17], and the fully end-to-end ClariNet [16]. Those\nmodels usually \ufb01rst generate mel-spectrogram autoregressively from text input and then synthesize\nspeech from the mel-spectrogram using vocoder such as Grif\ufb01n-Lim [6], WaveNet [21], Parallel\n\n\u2217Equal contribution\n\u2020Corresponding author\n3Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.\n\nPreprint. Under review.\n\n\fWaveNet [15], or WaveGlow [18]4. Neural network based TTS has outperformed conventional\nconcatenative and statistical parametric approaches [9, 25] in terms of speech quality.\n\nIn current neural network based TTS systems, mel-spectrogram is generated autoregressively. Due to\nthe long sequence of the mel-spectrogram and the autoregressive nature, those systems face several\nchallenges:\n\n\u2022 Slow inference speed for mel-spectrogram generation. Although CNN and Transformer\nbased TTS [13, 17] can speed up the training over RNN-based models [20], all models\ngenerate a mel-spectrogram conditioned on the previously generated mel-spectrograms and\nsuffer from slow inference speed, given the mel-spectrogram sequence is usually with a\nlength of hundreds or thousands.\n\n\u2022 Synthesized speech is usually not robust. Due to error propagation [3] and the wrong\nattention alignments between text and speech in the autoregressive generation, the generated\nmel-spectrogram is usually de\ufb01cient with the problem of words skipping and repeating [17].\n\u2022 Synthesized speech is lack of controllability. Previous autoregressive models generate\nmel-spectrograms one by one automatically, without explicitly leveraging the alignments\nbetween text and speech. As a consequence, it is usually hard to directly control the voice\nspeed and prosody in the autoregressive generation.\n\nConsidering the monotonous alignment between text and speech, to speed up mel-spectrogram\ngeneration, in this work, we propose a novel model, FastSpeech, which takes a text (phoneme)\nsequence as input and generates mel-spectrograms non-autoregressively. It adopts a feed-forward\nnetwork based on the self-attention in Transformer [22] and 1D convolution [5, 17]. Since a mel-\nspectrogram sequence is much longer than its corresponding phoneme sequence, in order to solve\nthe problem of length mismatch between the two sequences, FastSpeech adopts a length regulator\nthat up-samples the phoneme sequence according to the phoneme duration (i.e., the number of\nmel-spectrograms that each phoneme corresponds to) to match the length of the mel-spectrogram\nsequence. The regulator is built on a phoneme duration predictor, which predicts the duration of each\nphoneme.\n\nOur proposed FastSpeech can address the above-mentioned three challenges as follows:\n\n\u2022 Through parallel mel-spectrogram generation, FastSpeech greatly speeds up the synthesis\n\nprocess.\n\n\u2022 Phoneme duration predictor ensures hard alignments between a phoneme and its mel-\nspectrograms, which is very different from soft and automatic attention alignments in the\nautoregressive models. Thus, FastSpeech avoids the issues of error propagation and wrong\nattention alignments, consequently reducing the ratio of the skipped words and repeated\nwords.\n\n\u2022 The length regulator can easily adjust voice speed by lengthening or shortening the phoneme\nduration to determine the length of the generated mel-spectrograms, and can also control\npart of the prosody by adding breaks between adjacent phonemes.\n\nWe conduct experiments on the LJSpeech dataset to test FastSpeech. The results show that in terms\nof speech quality, FastSpeech nearly matches the autoregressive Transformer model. Furthermore,\nFastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on \ufb01nal speech\nsynthesis compared with the autoregressive Transformer TTS model, almost eliminates the problem\nof word skipping and repeating, and can adjust voice speed smoothly. We attach some audio \ufb01les\ngenerated by our method in the supplementary materials. We will release the codes once the paper is\npublished.\n\n2 Background\n\nIn this section, we brie\ufb02y overview the background of this work, including text to speech, sequence\nto sequence learning, and non-autoregressive sequence generation.\n\n4Although ClariNet [16] is fully end-to-end, it still \ufb01rst generates mel-spectrogram autoregressively and then\n\nsynthesizes speech in one model.\n\n2\n\n\fText to Speech TTS [1, 16, 19, 20, 24], which aims to synthesize natural and intelligible speech\ngiven text, has long been a hot research topic in the \ufb01eld of arti\ufb01cial intelligence. The research on\nTTS has shifted from early concatenative synthesis [9], statistical parametric synthesis [12, 25] to\nneural network based parametric synthesis [1] and end-to-end models [13, 16, 20, 24], and the quality\nof the synthesized speech by end-to-end models is close to human parity. Neural network based\nend-to-end TTS models usually \ufb01rst convert the text to acoustic features (e.g., mel-spectrograms) and\nthen transform mel-spectrograms into audio samples. However, most neural TTS systems generate\nmel-spectrograms autoregressively, which suffers from slow inference speed, and synthesized speech\nusually lacks of robustness (word skipping and repeating) and controllability (voice speed or prosody\ncontrol). In this work, we propose FastSpeech to generate mel-spectrograms non-autoregressively,\nwhich suf\ufb01ciently handles the above problems.\n\nSequence to Sequence Learning Sequence to sequence learning [2, 4, 22] is usually built on the\nencoder-decoder framework: The encoder takes the source sequence as input and generates a set of\nrepresentations. After that, the decoder estimates the conditional probability of each target element\ngiven the source representations and its preceding elements. The attention mechanism [2] is further\nintroduced between the encoder and decoder in order to \ufb01nd which source representations to focus\non when predicting the current element, and is an important component for sequence to sequence\nlearning.\n\nIn this work, instead of using the conventional encoder-attention-decoder framework for sequence to\nsequence learning, we propose a feed-forward network to generate a sequence in parallel.\n\nNon-Autoregressive Sequence Generation Unlike autoregressive sequence generation, non-\nautoregressive models generate sequence in parallel, without explicitly depending on the previous\nelements, which can greatly speed up the inference process. Non-autoregressive generation has\nbeen studied in some sequence generation tasks such as neural machine translation [7, 8, 23] and\naudio synthesis [15, 16, 18]. Our FastSpeech differs from the above works in two aspects: 1) Pre-\nvious works adopt non-autoregressive generation in neural machine translation or audio synthesis\nmainly for inference speedup, while FastSpeech focuses on both inference speedup and improving\nthe robustness and controllability of the synthesized speech in TTS. 2) For TTS, although Parallel\nWaveNet [15], ClariNet [16] and WaveGlow [18] generate audio in parallel, they are conditioned\non mel-spectrograms, which are still generated autoregressively. Therefore, they do not address the\nchallenges considered in this work.\n\n3 FastSpeech\n\nIn this section, we introduce the architecture design of FastSpeech. To generate a target mel-\nspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the\nencoder-attention-decoder based architecture as adopted by most sequence to sequence based autore-\ngressive [13, 20, 22] and non-autoregressive [7, 8, 23] generation. The overall model architecture of\nFastSpeech is shown in Figure 1. We describe the components in detail in the following subsections.\n\n3.1 Feed-Forward Transformer\n\nThe architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer [22]\nand 1D convolution [5, 17]. We call this structure as Feed-Forward Transformer (FFT), as shown in\nFigure 1a. Feed-Forward Transformer stacks multiple FFT blocks for phoneme to mel-spectrogram\ntransformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with\na length regulator (which will be described in the next subsection) in between to bridge the length gap\nbetween the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and\n1D convolutional network, as shown in Figure 1b. The self-attention network consists of a multi-head\nattention to extract the cross-position information. Different from the 2-layer dense network in\nTransformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that\nthe adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram\nsequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the\nexperimental section. Following Transformer [22], residual connections, layer normalization, and\ndropout are added after the self-attention network and 1D convolutional network respectively.\n\n3\n\n\f(a) Feed-Forward Transformer\n\n(b) FFT Block\n\n(c) Length Regulator\n\n(d) Duration Predictor\n\nFigure 1: The overall architecture for FastSpeech. (a). The feed-forward Transformer. (b). The\nfeed-forward Transformer block. (c). The length regulator. (d). The duration predictor. MSE loss\ndenotes the loss between predicted and extracted duration, which only exists in the training process.\n\n3.2 Length Regulator\n\nThe length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme\nand spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed and\npart of prosody. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram\nsequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of\nthe mel-spectrograms that corresponds to a phoneme as the phoneme duration (we will describe\nhow to predict phoneme duration in the next subsection). Based on the phoneme duration d, the\nlength regulator expands the hidden states of the phoneme sequence d times, and then the total length\nof the hidden states equals the length of the mel-spectrograms. Denote the hidden states of the\nphoneme sequence as Hpho = [h1, h2, ..., hn], where n is the length of the sequence. Denote the\nphoneme duration sequence as D = [d1, d2, ..., dn], where \u03a3n\ni=1di = m and m is the length of the\nmel-spectrogram sequence. We denote the length regulator LR as\n\nHmel = LR(Hpho, D, \u03b1),\n\n(1)\n\nwhere \u03b1 is a hyperparameter to determine the length of the expanded sequence Hmel, thereby\ncontrolling the voice speed. For example, given Hpho = [h1, h2, h3, h4] and the correspond-\ning phoneme duration sequence D = [2, 2, 3, 1], the expanded sequence Hmel based on Equa-\ntion 1 becomes [h1, h1, h2, h2, h3, h3, h3, h4] if \u03b1 = 1 (normal speed). When \u03b1 = 1.3 (slow\nspeed) and 0.5 (fast speed), the duration sequences become D\u03b1=1.3 = [2.6, 2.6, 3.9, 1.3] \u2248\n[3, 3, 4, 1] and D\u03b1=0.5 = [1, 1, 1.5, 0.5] \u2248 [1, 1, 2, 1], and the expanded sequences become\n[h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4] and [h1, h2, h3, h3, h4] respectively. We can also control\nthe break between words by adjusting the duration of the space characters in the sentence, so as to\nadjust part of prosody of the synthesized speech.\n\n3.3 Duration Predictor\n\nPhoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration\npredictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by\nthe layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\nis exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT\nblocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of\nmel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\nthe logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained\nduration predictor is only used in the TTS inference phase, because we can directly use the phoneme\nduration extracted from an autoregressive teacher model in training (see following discussions).\n\n4\n\nFFT BlockN xPhoneme EmbeddingPhonemeLength RegulatorN xLinear LayerFFT BlockMulti-Head AttentionAdd & NormConv1DAdd & NormDurationPredictor\ud835\udefc=1.0\ud835\udc9f=[2,2,3,1]AutoregressiveTransformer TTSDurationExtractorConv1D + NormPhonemeConv1D + NormLinear LayerMSE LossTraining\fIn order to train the duration predictor, we extract the ground-truth phoneme duration from an\nautoregressive teacher TTS model, as shown in Figure 1d. We describe the detailed steps as follows:\n\n\u2022 We \ufb01rst train an autoregressive encoder-attention-decoder based Transformer TTS model\n\nfollowing [13].\n\n\u2022 For each training sequence pair, we extract the decoder-to-encoder attention alignments\nfrom the trained teacher model. There are multiple attention alignments due to the multi-\nhead self-attention [22], and not all attention heads demonstrate the diagonal property (the\nphoneme and mel-spectrogram sequence are monotonously aligned). We propose a focus\nrate F to measure how an attention head is close to diagonal: F = 1\ns=1 max1\u2264t\u2264T as,t,\nS\nwhere S and T are the lengths of the ground-truth spectrograms and phonemes, as,t donates\nthe element in the s-th row and t-th column of the attention matrix. We compute the focus\nrate for each head and choose the head with the largest F as the attention alignments.\n\u2022 Finally, we extract the phoneme duration sequence D = [d1, d2, ..., dn] according to the\nduration extractor di = (cid:80)S\ns=1[arg maxt as,t = i]. That is, the duration of a phoneme is the\nnumber of mel-spectrograms attended to it according to the attention head selected in the\nabove step.\n\n(cid:80)S\n\n4 Experimental Setup\n\n4.1 Datasets\n\nWe conduct experiments on LJSpeech dataset [10], which contains 13,100 English audio clips and\nthe corresponding text transcripts, with the total audio length of approximate 24 hours. We randomly\nsplit the dataset into 3 sets: 12500 samples for training, 300 samples for validation and 300 samples\nfor testing. In order to alleviate the mispronunciation problem, we convert the text sequence into the\nphoneme sequence with our internal grapheme-to-phoneme conversion tool, following [1, 20, 24].\nFor the speech data, we convert the raw waveform into mel-spectrograms following [20]. Our frame\nsize and hop size are set to 1024 and 256, respectively.\n\nIn order to evaluate the robustness of our proposed FastSpeech, we also choose 50 sentences which\nare particularly hard for TTS system, following the practice in [17].\n\n4.2 Model Con\ufb01guration\n\nFastSpeech model Our FastSpeech model consists of 6 FFT blocks on both the phoneme side\nand the mel-spectrogram side. The size of the phoneme vocabulary is 51, including punctuations.\nThe dimension of phoneme embeddings, the hidden size of the self-attention and 1D convolution\nin the FFT block are all set to 384. The number of attention heads is set to 2. The kernel sizes of\nthe 1D convolution in the 2-layer convolutional network are both set to 3, with input/output size of\n384/1536 for the \ufb01rst layer and 1536/384 in the second layer. The output linear layer converts the\n384-dimensional hidden into 80-dimensional mel-spectrogram. In our duration predictor, the kernel\nsizes of the 1D convolution are set to 3, with input/output sizes of 384/384 for both layers.\n\nAutoregressive Transformer TTS model The autoregressive Transformer TTS model serves two\npurposes in our work: 1) to extract the phoneme duration as the target to train the duration predictor;\n2) to generate mel-spectrogram in the sequence-level knowledge distillation (which will be introduced\nin the next subsection). We follow [13] for the con\ufb01gurations of this model, which consists of a\n6-layer encoder, a 6-layer decoder, and a mel-postnet. The number of parameters of this teacher\nmodel is similar to that of our FastSpeech model.\n\n4.3 Training and Inference\n\nWe \ufb01rst train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs, with batchsize\nof 16 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10\u22129 and\nfollow the same learning rate schedule in [22]. It takes 80k steps for training until convergence. We\nfeed the text and speech pairs in the training set to the model again to obtain the encoder-decoder\nattention alignments, which are used to train the duration predictor. In addition, we also leverage\n\n5\n\n\fsequence-level knowledge distillation [11] that has achieved good performance in non-autoregressive\nmachine translation [7, 8, 23] to transfer the knowledge from the teacher model to the student model.\nFor each source text sequence, we generate the mel-spectrograms with the autoregressive Transformer\nTTS model and take the source text and the generated mel-spectrograms as the paired data for\nFastSpeech model training.\n\nWe train the FastSpeech model together with the duration predictor. The optimizer and other hyper-\nparameters for FastSpeech are the same as the autoregressive Transformer TTS model. In order\nto speed up the training process and improve performance, we initialize some parts of the weights\nfrom the autoregressive Transformer TTS model: 1) we initialize the phoneme embeddings from the\nautoregressive Transformer TTS model; 2) we also initialize the FFT blocks on the phoneme side\nwith the encoder of the autoregressive Transformer TTS model, as they share the same architecture.\nThe FastSpeech model training takes about 80k steps on 4 NVIDIA V100 GPUs. In the inference\nprocess, the output mel-spectrograms of our FastSpeech model are transformed into audio samples\nusing the pretrained WaveGlow [18]5.\n\n5 Results\n\nIn this section, we evaluate the performance of FastSpeech in terms of audio quality, inference\nspeedup, robustness, and controllability.\n\nAudio Quality We conduct the MOS (mean opinion score) evaluation on the test set to measure\nthe audio quality. We keep the text content consistent among different models so as to exclude other\ninterference factors, only examining the audio quality. Each audio is listened by at least 20 testers,\nwho are all native English speakers. We compare the MOS of the generated audio samples by our\nFastSpeech model with other systems, which include 1) GT, the ground truth audio; 2) GT (Mel +\nWaveGlow), where we \ufb01rst convert the ground truth audio into mel-spectrograms, and then convert\nthe mel-spectrograms back to audio using WaveGlow; 3) Tacotron 2 [20] (Mel + WaveGlow); 4)\nTransformer TTS [13] (Mel + WaveGlow). 5) Merlin [25] (WORLD), a popular parametric TTS\nsystem with WORLD [14] as the vocoder. The results are shown in Table 1. It can be seen that our\nFastSpeech can nearly match the quality of the Transformer TTS model and Tacotron 2.\n\nMethod\n\nGT\nGT (Mel + WaveGlow)\nTacotron 2 [20] (Mel + WaveGlow)\nMerlin [25] (WORLD)\n\nMOS\n\n4.41 \u00b1 0.08\n4.00 \u00b1 0.09\n3.86 \u00b1 0.09\n2.40 \u00b1 0.13\n\nTransformer TTS [13] (Mel + WaveGlow)\n\n3.88 \u00b1 0.09\n\nFastSpeech (Mel + WaveGlow)\n\n3.84 \u00b1 0.08\n\nTable 1: The MOS with 95% con\ufb01dence intervals.\n\nInference Speedup We evaluate the inference latency of FastSpeech compared with the autore-\ngressive Transformer TTS model, which has similar number of model parameters with FastSpeech.\nWe \ufb01rst show the inference speedup for mel-spectrogram generation in Table 2. It can be seen that\nFastSpeech speeds up the mel-spectrogram generation by 269.40x, compared with the Transformer\nTTS model. We then show the end-to-end speedup when using WaveGlow as the vocoder. It can be\nseen that FastSpeech can still achieve 38.30x speedup for audio generation.\n\nWe also visualize the relationship between the inference latency and the length of the predicted mel-\nspectrogram sequence in the test set. Figure 2 shows that the inference latency barely increases with\nthe length of the predicted mel-spectrogram for FastSpeech, while increases largely in Transformer\nTTS. This indicates that the inference speed of our method is not sensitive to the length of generated\naudio due to parallel generation.\n\n5https://github.com/NVIDIA/waveglow\n\n6\n\n\fMethod\n\nTransformer TTS [13] (Mel)\nFastSpeech (Mel)\n\nLatency (s)\n\nSpeedup\n\n6.735 \u00b1 3.969\n0.025 \u00b1 0.005\n\n/\n269.40\u00d7\n\nTransformer TTS [13] (Mel + WaveGlow)\nFastSpeech (Mel + WaveGlow)\n\n6.895 \u00b1 3.969\n0.180 \u00b1 0.078\n\n/\n38.30\u00d7\n\nTable 2: The comparison of inference latency with 95% con\ufb01dence intervals. The evaluation is\nconducted on a server with 12 Intel Xeon CPU, 256GB memory and 1 NVIDIA V100 GPU. The\naverage length of the generated mel-spectrograms for the two systems are both about 560.\n\n(a) FastSpeech\n\n(b) Transformer TTS\n\nFigure 2: Inference time (second) vs. mel-spectrogram length for FastSpeech and Transformer TTS.\n\nRobustness The encoder-decoder attention mechanism in the autoregressive model may cause\nwrong attention alignments between phoneme and mel-spectrogram, resulting in instability with word\nrepeating and word skipping. To evaluate the robustness of FastSpeech, we select 50 sentences which\nare particularly hard for TTS system6. Word error counts are listed in Table 3. It can be seen that\nTransformer TTS is not robust to these hard cases and gets 34% error rate, while FastSpeech can\neffectively eliminate word repeating and skipping to improve intelligibility.\n\nMethod\n\nRepeats\n\nSkips Error Sentences Error Rate\n\nTransformer TTS\nFastSpeech\n\n7\n0\n\n15\n0\n\n17\n0\n\n34%\n0%\n\nTable 3: The comparison of robustness between FastSpeech and Transformer TTS on the 50 particu-\nlarly hard sentences. Each kind of word error is counted at most once per sentence.\n\nLength Control As mentioned in Section 3.2, FastSpeech can control the voice speed as well as\npart of the prosody by adjusting the phoneme duration, which cannot be supported by other end-to-end\nTTS systems. We show the mel-spectrograms before and after the length control, and also put the\naudio samples in the supplementary material for reference.\n\nVoice Speed The generated mel-spectrograms with different voice speeds by lengthening or short-\nening the phoneme duration are shown in Figure 3. We also attach several audio samples in the\nsupplementary material for reference. As demonstrated by the samples, FastSpeech can adjust the\nvoice speed from 0.5x to 1.5x smoothly, with stable and almost unchanged pitch.\n\nBreaks Between Words FastSpeech can add breaks between adjacent words by lengthening the\nduration of the space characters in the sentence, which can improve the prosody of voice. We show\nan example in Figure 4, where we add breaks in two positions of the sentence to improve the prosody.\n\n6These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the\n\nsupplementary materials.\n\n7\n\n\f(a) 1.5x Voice Speed\n\n(b) 1.0x Voice Speed\n\n(c) 0.5x Voice Speed\n\nFigure 3: The mel-spectrograms of the voice with 1.5x, 1.0x and 0.5x speed respectively. The\ninput text is \"For a while the preacher addresses himself to the congregation at large, who listen\nattentively\".\n\n(a) Original Mel-spectrograms\n\n(b) Mel-spectrograms after adding breaks\n\nFigure 4: The mel-spectrograms before and after adding breaks between words. The corresponding\ntext is \"that he appeared to feel deeply the force of the reverend gentleman\u2019s observations, especially\nwhen the chaplain spoke of \". We add breaks after the words \"deeply\" and \"especially\" to improve the\nprosody. The red boxes in Figure 4b correspond to the added breaks.\n\nAblation Study We conduct ablation studies to verify the effectiveness of several components in\nFastSpeech, including 1D Convolution, sequence-level knowledge distillation and weight initialization\nfrom teacher model. We conduct CMOS evaluation for these ablation studies.\n\n1D Convolution in FFT Block We propose to replace the original fully connected layer (adopted in\nTransformer [22]) with 1D convolution in FFT block, as described in Section 3.1. Here we conduct\nexperiments to compare the performance of 1D convolution to the fully connected layer with similar\nnumber of parameters. As shown in Table 4, replacing 1D convolution with fully connected layer\nresults in -0.113 CMOS, which demonstrates the effectiveness of 1D convolution.\n\nSequence-Level Knowledge Distillation As described in Section 4.3, we leverage sequence-level\nknowledge distillation for FastSpeech. We conduct CMOS evaluation to compare the performance of\nFastSpeech with and without sequence-level knowledge distillation, as shown in Table 4. We \ufb01nd\nthat removing sequence-level knowledge distillation results in -0.325 CMOS, which demonstrates the\neffectiveness of sequence-level knowledge distillation.\n\nWeight Initialization from Teacher Model As mentioned in Section 4.3, we initialize part of the weights\nof our model with Transformer TTS. We also conduct the CMOS test to demonstrate the effectiveness\nof weight initialization, as shown in Table 4. We can see that removing weight initialization results in\n-0.061 CMOS. We also \ufb01nd that weight initialization speeds up the training process by nearly 1.5x.\n\nSystem\n\nFastSpeech\n\nFastSpeech without 1D convolution in FFT block\n\nFastSpeech without sequence-level knowledge distillation\n\nFastSpeech without weight initialization from teacher model\n\nTable 4: CMOS comparison in the ablation studies.\n\nCMOS\n\n0\n\n-0.113\n\n-0.325\n\n-0.061\n\n8\n", "Conclusion": "\n\nIn this work, we have proposed FastSpeech: a fast, robust and controllable neural TTS system.\nFastSpeech has a novel feed-forward network to generate mel-spectrogram in parallel, which consists\nof several key components including feed-forward Transformer blocks, a length regulator and a\nduration predictor. Experiments on LJSpeech dataset demonstrate that our proposed FastSpeech can\nnearly match the autoregressive Transformer TTS model in terms of speech quality, speed up the\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x, almost eliminate\nthe problem of word skipping and repeating, and can adjust voice speed (0.5x-1.5x) smoothly.\n\nFor future work, we will continue to improve the quality of the synthesized speech, and apply\nFastSpeech to multi-speaker and low-resource settings. We will also train FastSpeech jointly with a\nparallel neural vocoder to make it fully end-to-end and parallel.\n\n9\n", "References": "\n\n[1] Sercan O Arik, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-guo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-timeneural text-to-speech. arXiv preprint arXiv:1702.07825, 2017.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. ICLR 2015, 2015.\n\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling forIn Advances in Neural Informationsequence prediction with recurrent neural networks.Processing Systems, pages 1171\u20131179, 2015.\n\n[4] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neuralnetwork for large vocabulary conversational speech recognition. In Acoustics, Speech andSignal Processing (ICASSP), 2016 IEEE International Conference on, pages 4960\u20134964. IEEE,2016.\n\n[5] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutionalsequence to sequence learning. In Proceedings of the 34th International Conference on MachineLearning-Volume 70, pages 1243\u20131252. JMLR. org, 2017.\n\n[6] Daniel Grif\ufb01n and Jae Lim. Signal estimation from modi\ufb01ed short-time fourier transform. IEEETransactions on Acoustics, Speech, and Signal Processing, 32(2):236\u2013243, 1984.\n\n[7] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017.\n\n[8] Junliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and Tie-Yan Liu. Non-autoregressive neuralmachine translation with enhanced decoder input. In AAAI, 2019.\n\n[9] Andrew J Hunt and Alan W Black. Unit selection in a concatenative speech synthesis systemusing a large speech database. In 1996 IEEE International Conference on Acoustics, Speech,and Signal Processing Conference Proceedings, volume 1, pages 373\u2013376. IEEE, 1996.\n\n[10] Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.\n\n[11] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprintarXiv:1606.07947, 2016.\n\n[12] Hao Li, Yongguo Kang, and Zhenyu Wang. Emphasis: An emotional phoneme-based acousticmodel for speech synthesis system. arXiv preprint arXiv:1806.09276, 2018.\n\n[13] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to humanquality tts with transformer. arXiv preprint arXiv:1809.08895, 2018.\n\n[14] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-qualityspeech synthesis system for real-time applications. IEICE TRANSACTIONS on Informationand Systems, 99(7):1877\u20131884, 2016.\n\n[15] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, KorayKavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg,et al. Parallel wavenet: Fast high-\ufb01delity speech synthesis. arXiv preprint arXiv:1711.10433,2017.\n\n[16] Wei Ping, Kainan Peng, and Jitong Chen. Clarinet: Parallel wave generation in end-to-endtext-to-speech. In International Conference on Learning Representations, 2019.\n\n[17] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. InInternational Conference on Learning Representations, 2018.\n\n[18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative networkfor speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.10\f"}